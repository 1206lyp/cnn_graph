{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NFEATURES = 28**2\n",
    "NCLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's common API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class base_model(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.regularizers = 0\n",
    "    \n",
    "    def loss(self, logits, labels, regularization):\n",
    "        labels = tf.to_int64(labels)\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        loss += regularization * self.regularizers\n",
    "        return loss\n",
    "    \n",
    "    def training(self, loss, learning_rate):\n",
    "        tf.scalar_summary(loss.op.name, loss)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op\n",
    "    \n",
    "    def evaluation(self, logits, labels):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    \n",
    "    # Helpers\n",
    "    \n",
    "    def _weight_variable(self, shape, regularization=True):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        var = tf.Variable(initial, name='weights')\n",
    "        if regularization:\n",
    "            self.regularizers += tf.nn.l2_loss(var)\n",
    "        return var\n",
    "\n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        var = tf.Variable(initial, name='bias')\n",
    "        if regularization:\n",
    "            self.regularizers += tf.nn.l2_loss(var)\n",
    "        return var\n",
    "\n",
    "    def _conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fc1(base_model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def inference(self, x):\n",
    "        W = self._weight_variable([NFEATURES, NCLASSES])\n",
    "        b = self._bias_variable([NCLASSES])\n",
    "        y = tf.matmul(x, W) + b\n",
    "        return y\n",
    "\n",
    "class fc2(base_model):\n",
    "    def __init__(self, nhiddens):\n",
    "        super().__init__()\n",
    "        self.nhiddens = nhiddens\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([NFEATURES, self.nhiddens])\n",
    "            b = self._bias_variable([self.nhiddens])\n",
    "            y = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "        with tf.name_scope('fc2'):\n",
    "            W = self._weight_variable([self.nhiddens, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cnn2(base_model):\n",
    "    \"\"\"Simple convolutional model.\"\"\"\n",
    "    def __init__(self, K, F):\n",
    "        super().__init__()\n",
    "        self.K = K  # Patch size\n",
    "        self.F = F  # Number of features\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('conv1'):\n",
    "            W = self._weight_variable([self.K, self.K, 1, self.F])\n",
    "            b = self._bias_variable([self.F])\n",
    "#            b = self._bias_variable([1, 28, 28, self.F])\n",
    "            x_2d = tf.reshape(x, [-1,28,28,1])\n",
    "            y_2d = self._conv2d(x_2d, W) + b\n",
    "            y_2d = tf.nn.relu(y_2d)\n",
    "        with tf.name_scope('fc1'):\n",
    "            y = tf.reshape(y_2d, [-1, NFEATURES*self.F])\n",
    "            W = self._weight_variable([NFEATURES*self.F, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class fcnn2(base_model):\n",
    "    \"\"\"CNN using the FFT.\"\"\"\n",
    "    def __init__(self, F):\n",
    "        super().__init__()\n",
    "        self.F = F  # Number of features\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('conv1'):\n",
    "            # Transform to Fourier domain\n",
    "            x_2d = tf.reshape(x, [-1, 28, 28])\n",
    "            x_2d = tf.complex(x_2d, 0)\n",
    "            xf_2d = tf.batch_fft2d(x_2d)\n",
    "            xf = tf.reshape(xf_2d, [-1, NFEATURES])\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            Wreal = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            Wimg = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            W = tf.complex(Wreal, Wimg)\n",
    "            xf = xf[:int(NFEATURES/2), :, :]\n",
    "            yf = tf.batch_matmul(W, xf)  # for each feature\n",
    "            yf = tf.concat(0, [yf, tf.conj(yf)])\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf_2d = tf.reshape(yf, [-1, 28, 28])\n",
    "            # Transform back to spatial domain\n",
    "            y_2d = tf.batch_ifft2d(yf_2d)\n",
    "            y_2d = tf.real(y_2d)\n",
    "            y = tf.reshape(y_2d, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fgcnn2(base_model):\n",
    "    \"\"\"Graph CNN with full weights, i.e. patch has the same size as input.\"\"\"\n",
    "    def __init__(self, L, F):\n",
    "        super().__init__()\n",
    "        #self.L = L  # Graph Laplacian, NFEATURES x NFEATURES\n",
    "        self.F = F  # Number of filters\n",
    "        _, self.U = graph.fourier(L)\n",
    "    def inference(self, x):\n",
    "        # x: NSAMPLES x NFEATURES\n",
    "        with tf.name_scope('gconv1'):\n",
    "            # Transform to Fourier domain\n",
    "            U = tf.constant(self.U, dtype=tf.float32)\n",
    "            xf = tf.matmul(x, U)\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            W = self._weight_variable([NFEATURES, self.F, 1])\n",
    "            yf = tf.batch_matmul(W, xf)  # for each feature\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf = tf.reshape(yf, [-1, NFEATURES])\n",
    "            # Transform back to graph domain\n",
    "            Ut = tf.transpose(U)\n",
    "            y = tf.matmul(yf, Ut)\n",
    "            y = tf.reshape(yf, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lgcnn2_1(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M, K = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.reshape(x, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, 1, self.F])\n",
    "#            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class lgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.transpose(x)  # M x N\n",
    "            def lanczos(x):\n",
    "                return graph.lanczos(self.L, x, self.K)\n",
    "            xl = tf.py_func(lanczos, [xl], [tf.float32])[0]\n",
    "            xl = tf.transpose(xl)  # N x M x K\n",
    "            xl = tf.reshape(xl, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xc = tf.transpose(x)  # M x N\n",
    "            def chebyshev(x):\n",
    "                return graph.chebyshev(self.L, x, self.K)\n",
    "            xc = tf.py_func(chebyshev, [xc], [tf.float32])[0]\n",
    "            xc = tf.transpose(xc)  # N x M x K\n",
    "            xc = tf.reshape(xc, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xc, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_3(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.L = L.toarray()\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = x\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.matmul(x, self.L, b_is_sparse=True)  # N x M\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.matmul(xt1, self.L, b_is_sparse=True) - xt0  # N x M\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_4(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.transpose(xt)  # N x M\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_5(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            xt = tf.expand_dims(xt0, 0)  # 1 x M x N\n",
    "            def concat(xt, x):\n",
    "                x = tf.expand_dims(x, 0)  # 1 x M x N\n",
    "                return tf.concat(0, [xt, x])  # K x M x N\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                xt = concat(xt, xt1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                xt = concat(xt, xt2)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            xt = tf.transpose(xt)  # N x M x K\n",
    "            xt = tf.reshape(xt, [-1,self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xt, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cgcnn(base_model):\n",
    "    \"\"\"\n",
    "    Graph CNN which uses the Chebyshev approximation.\n",
    "\n",
    "    The following are hyper-parameters of graph convolutional layers.\n",
    "    They are lists, which length is equal to the number of gconv layers.\n",
    "        F: Number of features.\n",
    "        K: List of polynomial orders, i.e. filter sizes or number of hopes.\n",
    "        p: Pooling size.\n",
    "           Should be 1 (no pooling) or a power of 2 (reduction by 2 at each coarser level).\n",
    "           Beware to have coarsened enough.\n",
    "\n",
    "    L: List of Graph Laplacians. Size M x M. One per coarsening level.\n",
    "\n",
    "    The following are hyper-parameters of fully connected layers.\n",
    "    They are lists, which length is equal to the number of fc layers.\n",
    "        M: Number of features per sample, i.e. number of hidden neurons.\n",
    "    \"\"\"\n",
    "    def __init__(self, L, F, K, p, M):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(L) >= len(F) == len(K) == len(p)\n",
    "        assert np.all(np.array(p) >= 1)\n",
    "        p_log2 = np.where(np.array(p) > 1, np.log2(p), 0)\n",
    "        assert np.all(np.mod(p_log2, 1) == 0)  # Powers of 2.\n",
    "        assert len(L) >= 1 + np.sum(p_log2)  # Enough coarsening levels for pool sizes.\n",
    "        \n",
    "        # Keep the useful Laplacians only.\n",
    "        j = 0\n",
    "        self.L = []\n",
    "        for i in range(len(p)):\n",
    "            self.L.append(L[j])\n",
    "            j += int(np.log2(p[i])) if p[i] > 1 else 0\n",
    "        for i in range(len(self.L)):\n",
    "            self.L[i] = graph.rescale_L(self.L[i], lmax=2)\n",
    "        del L  # Prevent further usage.\n",
    "        \n",
    "        self.F, self.K, self.p, self.M = F, K, p, M\n",
    "        \n",
    "        Ngconv = len(p)\n",
    "        Nfc = len(M)\n",
    "        print('NN architecture')\n",
    "        print('  input: M_0 = {}'.format(self.L[0].shape[0]))\n",
    "        for i in range(Ngconv):\n",
    "            M_i = self.L[i].shape[0]\n",
    "            print('  layer {0}: cgconv{0}'.format(i+1))\n",
    "            print('    representation: M_{0} * F_{0} / p_{0} = {1} * {2} / {3} = {4}'.format(\n",
    "                    i+1, M_i, F[i], p[i], M_i*F[i]//p[i]))\n",
    "            F_last = F[i-1] if i > 0 else 1\n",
    "            print('    parameters: F_{1} * F_{0} * K_{0} = {2} * {3} * {4} = {5}'.format(\n",
    "                    i+1, i, F_last, F[i], K[i], F_last*F[i]*K[i]))\n",
    "        def M_last(i):\n",
    "            return self.M[i-1] if i > 0 else self.L[-1].shape[0] * F[-1] // p[-1]\n",
    "        for i in range(Nfc):\n",
    "            print('  layer {}: fc{}'.format(Ngconv+i+1, i+1))\n",
    "            print('    representation: M_{} = {}'.format(Ngconv+i+1, self.M[i]))\n",
    "            print('    parameters: M_{} * M_{} = {} * {} = {}'.format(\n",
    "                    Ngconv+i, Ngconv+i+1, M_last(i), M[i], M_last(i)*M[i]))\n",
    "        print('  layer {}: softmax'.format(Ngconv+Nfc+1))\n",
    "        print('    representation: M_{} = {}'.format(Ngconv+Nfc+1, NCLASSES))\n",
    "        print('    parameters: M_{} * M_{} = {} * {} = {}'.format(\n",
    "                Ngconv+Nfc, Ngconv+Nfc+1, M_last(Nfc), NCLASSES, M_last(Nfc)*NCLASSES))\n",
    "\n",
    "    def chebyshev2(self, x, L, Fout, K):\n",
    "        \"\"\"\n",
    "        Filtering with Chebyshev interpolation\n",
    "        Implementation: numpy.\n",
    "        \n",
    "        Data: x of size N x M x F\n",
    "            N: number of signals\n",
    "            M: number of vertices\n",
    "            F: number of features per signal per vertex\n",
    "        \"\"\"\n",
    "        N, M, Fin = x.get_shape()\n",
    "        M, Fin = int(M), int(Fin)\n",
    "        # Transform to Chebyshev basis\n",
    "        x = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
    "        x = tf.reshape(x, [M, -1])  # M x Fin*N\n",
    "        def chebyshev(x):\n",
    "            return graph.chebyshev(L, x, K)\n",
    "        x = tf.py_func(chebyshev, [x], [tf.float32])[0]  # K x M x Fin*N\n",
    "        x = tf.reshape(x, [K, M, Fin, -1])  # K*Fin x M*N\n",
    "        x = tf.transpose(x, perm=[3,1,2,0])  # N x M x Fin x K\n",
    "        x = tf.reshape(x, [-1, Fin*K])  # N*M x Fin*K\n",
    "        # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature.\n",
    "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "        x = tf.matmul(x, W)  # NM x Fout\n",
    "        return tf.reshape(x, [-1, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def b1relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per vertex per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, int(M), int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def b2relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def mpool1(self, x, p):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            #tf.maximum\n",
    "            return tf.squeeze(x, [3])  # N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def fc(self, x, Mout, relu=True):\n",
    "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
    "        N, Min = x.get_shape()\n",
    "        W = self._weight_variable([int(Min), Mout], regularization=True)\n",
    "        b = self._bias_variable([Mout], regularization=True)\n",
    "        x = tf.matmul(x, W) + b\n",
    "        if relu:\n",
    "            return tf.nn.relu(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def inference(self, x):\n",
    "        \"\"\"\n",
    "        Infer the logits given raw data.\n",
    "        \n",
    "        Data: x of size N x M\n",
    "            N: number of signals\n",
    "            M: number of vertices\n",
    "        \"\"\"\n",
    "        # Graph convolutional layers.\n",
    "        x = tf.expand_dims(x, 2)  # N x M x F=1\n",
    "        for i in range(len(self.p)):\n",
    "            with tf.name_scope('cgconv{}'.format(i)):\n",
    "                x = self.chebyshev2(x, self.L[i], self.F[i], self.K[i])\n",
    "                x = self.b1relu(x)\n",
    "                x = self.mpool1(x, self.p[i])\n",
    "        \n",
    "        # Fully connected hidden layers.\n",
    "        M = self.L[i].shape[0] // self.p[i] * self.F[-1]\n",
    "        x = tf.reshape(x, [-1, M])  # N x M\n",
    "        for i in range(len(self.M)):\n",
    "            with tf.name_scope('fc{}'.format(i)):\n",
    "                x = self.fc(x, self.M[i])\n",
    "        \n",
    "        # Logits linear layer, i.e. softmax without normalization.\n",
    "        with tf.name_scope('logits'):\n",
    "            x = self.fc(x, NCLASSES, False)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
