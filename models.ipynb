{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NFEATURES = 28**2\n",
    "NCLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's common API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class base_model(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.regularizers = 0\n",
    "    \n",
    "    def loss(self, logits, labels, regularization):\n",
    "        labels = tf.to_int64(labels)\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        loss += regularization * self.regularizers\n",
    "        return loss\n",
    "    \n",
    "    def training(self, loss, learning_rate):\n",
    "        tf.scalar_summary(loss.op.name, loss)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op\n",
    "    \n",
    "    def evaluation(self, logits, labels):\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    \n",
    "    # Helpers\n",
    "    \n",
    "    def _weight_variable(self, shape, regularization=True):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        var = tf.Variable(initial, name='weights')\n",
    "        if regularization:\n",
    "            self.regularizers += tf.nn.l2_loss(var)\n",
    "        return var\n",
    "\n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        var = tf.Variable(initial, name='bias')\n",
    "        if regularization:\n",
    "            self.regularizers += tf.nn.l2_loss(var)\n",
    "        return var\n",
    "\n",
    "    def _conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fc1(base_model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def inference(self, x):\n",
    "        W = self._weight_variable([NFEATURES, NCLASSES])\n",
    "        b = self._bias_variable([NCLASSES])\n",
    "        y = tf.matmul(x, W) + b\n",
    "        return y\n",
    "\n",
    "class fc2(base_model):\n",
    "    def __init__(self, nhiddens):\n",
    "        super().__init__()\n",
    "        self.nhiddens = nhiddens\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([NFEATURES, self.nhiddens])\n",
    "            b = self._bias_variable([self.nhiddens])\n",
    "            y = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "        with tf.name_scope('fc2'):\n",
    "            W = self._weight_variable([self.nhiddens, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cnn2(base_model):\n",
    "    \"\"\"Simple convolutional model.\"\"\"\n",
    "    def __init__(self, K, F):\n",
    "        super().__init__()\n",
    "        self.K = K  # Patch size\n",
    "        self.F = F  # Number of features\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('conv1'):\n",
    "            W = self._weight_variable([self.K, self.K, 1, self.F])\n",
    "            b = self._bias_variable([self.F])\n",
    "#            b = self._bias_variable([1, 28, 28, self.F])\n",
    "            x_2d = tf.reshape(x, [-1,28,28,1])\n",
    "            y_2d = self._conv2d(x_2d, W) + b\n",
    "            y_2d = tf.nn.relu(y_2d)\n",
    "        with tf.name_scope('fc1'):\n",
    "            y = tf.reshape(y_2d, [-1, NFEATURES*self.F])\n",
    "            W = self._weight_variable([NFEATURES*self.F, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class fcnn2(base_model):\n",
    "    \"\"\"CNN using the FFT.\"\"\"\n",
    "    def __init__(self, F):\n",
    "        super().__init__()\n",
    "        self.F = F  # Number of features\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('conv1'):\n",
    "            # Transform to Fourier domain\n",
    "            x_2d = tf.reshape(x, [-1, 28, 28])\n",
    "            x_2d = tf.complex(x_2d, 0)\n",
    "            xf_2d = tf.batch_fft2d(x_2d)\n",
    "            xf = tf.reshape(xf_2d, [-1, NFEATURES])\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            Wreal = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            Wimg = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            W = tf.complex(Wreal, Wimg)\n",
    "            xf = xf[:int(NFEATURES/2), :, :]\n",
    "            yf = tf.batch_matmul(W, xf)  # for each feature\n",
    "            yf = tf.concat(0, [yf, tf.conj(yf)])\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf_2d = tf.reshape(yf, [-1, 28, 28])\n",
    "            # Transform back to spatial domain\n",
    "            y_2d = tf.batch_ifft2d(yf_2d)\n",
    "            y_2d = tf.real(y_2d)\n",
    "            y = tf.reshape(y_2d, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fgcnn2(base_model):\n",
    "    \"\"\"Graph CNN with full weights, i.e. patch has the same size as input.\"\"\"\n",
    "    def __init__(self, L, F):\n",
    "        super().__init__()\n",
    "        #self.L = L  # Graph Laplacian, NFEATURES x NFEATURES\n",
    "        self.F = F  # Number of filters\n",
    "        _, self.U = graph.fourier(L)\n",
    "    def inference(self, x):\n",
    "        # x: NSAMPLES x NFEATURES\n",
    "        with tf.name_scope('gconv1'):\n",
    "            # Transform to Fourier domain\n",
    "            U = tf.constant(self.U, dtype=tf.float32)\n",
    "            xf = tf.matmul(x, U)\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            W = self._weight_variable([NFEATURES, self.F, 1])\n",
    "            yf = tf.batch_matmul(W, xf)  # for each feature\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf = tf.reshape(yf, [-1, NFEATURES])\n",
    "            # Transform back to graph domain\n",
    "            Ut = tf.transpose(U)\n",
    "            y = tf.matmul(yf, Ut)\n",
    "            y = tf.reshape(yf, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lgcnn2_1(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M, K = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.reshape(x, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, 1, self.F])\n",
    "#            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class lgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.transpose(x)  # M x N\n",
    "            def lanczos(x):\n",
    "                return graph.lanczos(self.L, x, self.K)\n",
    "            xl = tf.py_func(lanczos, [xl], [tf.float32])[0]\n",
    "            xl = tf.transpose(xl)  # N x M x K\n",
    "            xl = tf.reshape(xl, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xc = tf.transpose(x)  # M x N\n",
    "            def chebyshev(x):\n",
    "                return graph.chebyshev(self.L, x, self.K)\n",
    "            xc = tf.py_func(chebyshev, [xc], [tf.float32])[0]\n",
    "            xc = tf.transpose(xc)  # N x M x K\n",
    "            xc = tf.reshape(xc, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xc, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_3(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.L = L.toarray()\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = x\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.matmul(x, self.L, b_is_sparse=True)  # N x M\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.matmul(xt1, self.L, b_is_sparse=True) - xt0  # N x M\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_4(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.transpose(xt)  # N x M\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_5(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def inference(self, x):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            xt = tf.expand_dims(xt0, 0)  # 1 x M x N\n",
    "            def concat(xt, x):\n",
    "                x = tf.expand_dims(x, 0)  # 1 x M x N\n",
    "                return tf.concat(0, [xt, x])  # K x M x N\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                xt = concat(xt, xt1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                xt = concat(xt, xt2)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            xt = tf.transpose(xt)  # N x M x K\n",
    "            xt = tf.reshape(xt, [-1,self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xt, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
