{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NFEATURES = 28**2\n",
    "NFEATURES = 1000\n",
    "#NCLASSES = 10\n",
    "NCLASSES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common methods for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class base_model(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.regularizers = []\n",
    "    \n",
    "    def inference(self, data, dropout):\n",
    "        \"\"\"\n",
    "        It builds the model, i.e. the computational graph, as far as\n",
    "        is required for running the network forward to make predictions,\n",
    "        i.e. return logits given raw data.\n",
    "\n",
    "        data: size N x M\n",
    "            N: number of signals (samples)\n",
    "            M: number of vertices (features)\n",
    "        training: we may want to discriminate the two, e.g. for dropout.\n",
    "            True: the model is built for training.\n",
    "            False: the model is built for evaluation.\n",
    "        \"\"\"\n",
    "        logits = self._inference(data, dropout)\n",
    "        return logits\n",
    "    \n",
    "    def prediction(self, logits, labels):\n",
    "        \"\"\"Return the probability of a sample to belong to each class.\"\"\"\n",
    "        with tf.name_scope('prediction'):\n",
    "            predictions = tf.nn.softmax(logits)\n",
    "            return predictions\n",
    "\n",
    "    def loss(self, logits, labels, regularization):\n",
    "        \"\"\"Adds to the inference model the layers required to generate loss.\"\"\"\n",
    "        with tf.name_scope('loss'):\n",
    "            with tf.name_scope('cross_entropy'):\n",
    "                labels = tf.to_int64(labels)\n",
    "                cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "                cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "            with tf.name_scope('regularization'):\n",
    "                regularization *= tf.add_n(self.regularizers)\n",
    "            loss = cross_entropy + regularization\n",
    "            \n",
    "            # Summaries for TensorBoard.\n",
    "            tf.scalar_summary('loss/cross_entropy', cross_entropy)\n",
    "            tf.scalar_summary('loss/regularization', regularization)\n",
    "            tf.scalar_summary('loss/total', loss)\n",
    "            with tf.name_scope('averages'):\n",
    "                averages = tf.train.ExponentialMovingAverage(0.9)\n",
    "                op_averages = averages.apply([cross_entropy, regularization, loss])\n",
    "                tf.scalar_summary('loss/avg/cross_entropy', averages.average(cross_entropy))\n",
    "                tf.scalar_summary('loss/avg/regularization', averages.average(regularization))\n",
    "                tf.scalar_summary('loss/avg/total', averages.average(loss))\n",
    "                with tf.control_dependencies([op_averages]):\n",
    "                    loss_average = tf.identity(averages.average(loss), name='control')\n",
    "            return loss, loss_average\n",
    "    \n",
    "    def training(self, loss, learning_rate, decay_step, decay_rate=0.95, momentum=0.9):\n",
    "        \"\"\"Adds to the loss model the Ops required to generate and apply gradients.\"\"\"\n",
    "        with tf.name_scope('training'):\n",
    "            # Learning rate.\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            if decay_rate != 1:\n",
    "                learning_rate = tf.train.exponential_decay(\n",
    "                        learning_rate, global_step, decay_step, decay_rate, staircase=True)\n",
    "            tf.scalar_summary('learning_rate', learning_rate)\n",
    "            # Optimizer.\n",
    "            if momentum == 0:\n",
    "                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "            else:\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "            grads = optimizer.compute_gradients(loss)\n",
    "            op_gradients = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "            # Histograms.\n",
    "            for grad, var in grads:\n",
    "                tf.histogram_summary(var.op.name + '/gradients', grad)\n",
    "            # The op return the learning rate.\n",
    "            with tf.control_dependencies([op_gradients]):\n",
    "                op_train = tf.identity(learning_rate, name='control')\n",
    "            return op_train\n",
    "    \n",
    "    def evaluation(self, logits, labels):\n",
    "        \"\"\"Return the number of correct predictions.\"\"\"\n",
    "        with tf.name_scope('evaluation'):\n",
    "            correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "            ncorrects = tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "            return ncorrects\n",
    "\n",
    "    # Helpers\n",
    "\n",
    "    def _weight_variable(self, shape, regularization=True):\n",
    "        initial = tf.truncated_normal_initializer(0, 0.1)\n",
    "        var = tf.get_variable('weights', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "        return var\n",
    "\n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        initial = tf.constant_initializer(0.1)\n",
    "        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "        return var\n",
    "\n",
    "    def _conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fc1(base_model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def _inference(self, x, dropout):\n",
    "        W = self._weight_variable([NFEATURES, NCLASSES])\n",
    "        b = self._bias_variable([NCLASSES])\n",
    "        y = tf.matmul(x, W) + b\n",
    "        return y\n",
    "\n",
    "class fc2(base_model):\n",
    "    def __init__(self, nhiddens):\n",
    "        super().__init__()\n",
    "        self.nhiddens = nhiddens\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([NFEATURES, self.nhiddens])\n",
    "            b = self._bias_variable([self.nhiddens])\n",
    "            y = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "        with tf.name_scope('fc2'):\n",
    "            W = self._weight_variable([self.nhiddens, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cnn2(base_model):\n",
    "    \"\"\"Simple convolutional model.\"\"\"\n",
    "    def __init__(self, K, F):\n",
    "        super().__init__()\n",
    "        self.K = K  # Patch size\n",
    "        self.F = F  # Number of features\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('conv1'):\n",
    "            W = self._weight_variable([self.K, self.K, 1, self.F])\n",
    "            b = self._bias_variable([self.F])\n",
    "#            b = self._bias_variable([1, 28, 28, self.F])\n",
    "            x_2d = tf.reshape(x, [-1,28,28,1])\n",
    "            y_2d = self._conv2d(x_2d, W) + b\n",
    "            y_2d = tf.nn.relu(y_2d)\n",
    "        with tf.name_scope('fc1'):\n",
    "            y = tf.reshape(y_2d, [-1, NFEATURES*self.F])\n",
    "            W = self._weight_variable([NFEATURES*self.F, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class fcnn2(base_model):\n",
    "    \"\"\"CNN using the FFT.\"\"\"\n",
    "    def __init__(self, F):\n",
    "        super().__init__()\n",
    "        self.F = F  # Number of features\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('conv1'):\n",
    "            # Transform to Fourier domain\n",
    "            x_2d = tf.reshape(x, [-1, 28, 28])\n",
    "            x_2d = tf.complex(x_2d, 0)\n",
    "            xf_2d = tf.batch_fft2d(x_2d)\n",
    "            xf = tf.reshape(xf_2d, [-1, NFEATURES])\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            Wreal = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            Wimg = self._weight_variable([int(NFEATURES/2), self.F, 1])\n",
    "            W = tf.complex(Wreal, Wimg)\n",
    "            xf = xf[:int(NFEATURES/2), :, :]\n",
    "            yf = tf.batch_matmul(W, xf)  # for each feature\n",
    "            yf = tf.concat(0, [yf, tf.conj(yf)])\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf_2d = tf.reshape(yf, [-1, 28, 28])\n",
    "            # Transform back to spatial domain\n",
    "            y_2d = tf.batch_ifft2d(yf_2d)\n",
    "            y_2d = tf.real(y_2d)\n",
    "            y = tf.reshape(y_2d, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fgcnn2(base_model):\n",
    "    \"\"\"Graph CNN with full weights, i.e. patch has the same size as input.\"\"\"\n",
    "    def __init__(self, L, F):\n",
    "        super().__init__()\n",
    "        #self.L = L  # Graph Laplacian, NFEATURES x NFEATURES\n",
    "        self.F = F  # Number of filters\n",
    "        _, self.U = graph.fourier(L)\n",
    "    def _inference(self, x, dropout):\n",
    "        # x: NSAMPLES x NFEATURES\n",
    "        with tf.name_scope('gconv1'):\n",
    "            # Transform to Fourier domain\n",
    "            U = tf.constant(self.U, dtype=tf.float32)\n",
    "            xf = tf.matmul(x, U)\n",
    "            xf = tf.expand_dims(xf, 1)  # NSAMPLES x 1 x NFEATURES\n",
    "            xf = tf.transpose(xf)  # NFEATURES x 1 x NSAMPLES\n",
    "            # Filter\n",
    "            W = self._weight_variable([NFEATURES, self.F, 1])\n",
    "            yf = tf.batch_matmul(W, xf)  # for each feature\n",
    "            yf = tf.transpose(yf)  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            yf = tf.reshape(yf, [-1, NFEATURES])\n",
    "            # Transform back to graph domain\n",
    "            Ut = tf.transpose(U)\n",
    "            y = tf.matmul(yf, Ut)\n",
    "            y = tf.reshape(yf, [-1, self.F, NFEATURES])\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, self.F, 1])\n",
    "#            b = self._bias_variable([1, self.F, NFEATURES])\n",
    "            y += b  # NSAMPLES x NFILTERS x NFEATURES\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*NFEATURES, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*NFEATURES])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lgcnn2_1(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M, K = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.reshape(x, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "            b = self._bias_variable([1, 1, self.F])\n",
    "#            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class lgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Lanczos approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = L  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Lanczos basis\n",
    "            xl = tf.transpose(x)  # M x N\n",
    "            def lanczos(x):\n",
    "                return graph.lanczos(self.L, x, self.K)\n",
    "            xl = tf.py_func(lanczos, [xl], [tf.float32])[0]\n",
    "            xl = tf.transpose(xl)  # N x M x K\n",
    "            xl = tf.reshape(xl, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xl, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cgcnn2_2(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        self.L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xc = tf.transpose(x)  # M x N\n",
    "            def chebyshev(x):\n",
    "                return graph.chebyshev(self.L, x, self.K)\n",
    "            xc = tf.py_func(chebyshev, [xc], [tf.float32])[0]\n",
    "            xc = tf.transpose(xc)  # N x M x K\n",
    "            xc = tf.reshape(xc, [-1, self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xc, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_3(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        self.L = L.toarray()\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = x\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.matmul(x, self.L, b_is_sparse=True)  # N x M\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.matmul(xt1, self.L, b_is_sparse=True) - xt0  # N x M\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_4(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            def filter(xt, k):\n",
    "                xt = tf.transpose(xt)  # N x M\n",
    "                xt = tf.reshape(xt, [-1, 1])  # NM x 1\n",
    "                w = tf.slice(W, [k,0], [1,-1])  # 1 x F\n",
    "                y = tf.matmul(xt, w)  # NM x F\n",
    "                return tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            y = filter(xt0, 0)\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                y += filter(xt1, 1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                y += filter(xt2, k)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y\n",
    "\n",
    "class cgcnn2_5(base_model):\n",
    "    \"\"\"Graph CNN which uses the Chebyshev approximation.\"\"\"\n",
    "    def __init__(self, L, F, K):\n",
    "        super().__init__()\n",
    "        L = graph.rescale_L(L, lmax=2)  # Graph Laplacian, M x M\n",
    "        L = L.tocoo()\n",
    "        data = L.data\n",
    "        indices = np.empty((L.nnz, 2))\n",
    "        indices[:,0] = L.row\n",
    "        indices[:,1] = L.col\n",
    "        L = tf.SparseTensor(indices, data, L.shape)\n",
    "        self.L = tf.sparse_reorder(L)\n",
    "        self.F = F  # Number of filters\n",
    "        self.K = K  # Polynomial order, i.e. filter size (number of hopes)\n",
    "    def _inference(self, x, dropout):\n",
    "        with tf.name_scope('gconv1'):\n",
    "            N, M = x.get_shape()  # N: number of samples, M: number of features\n",
    "            M = int(M)\n",
    "            # Transform to Chebyshev basis\n",
    "            xt0 = tf.transpose(x)  # M x N\n",
    "            xt = tf.expand_dims(xt0, 0)  # 1 x M x N\n",
    "            def concat(xt, x):\n",
    "                x = tf.expand_dims(x, 0)  # 1 x M x N\n",
    "                return tf.concat(0, [xt, x])  # K x M x N\n",
    "            if self.K > 1:\n",
    "                xt1 = tf.sparse_tensor_dense_matmul(self.L, xt0)\n",
    "                xt = concat(xt, xt1)\n",
    "            for k in range(2, self.K):\n",
    "                xt2 = 2 * tf.sparse_tensor_dense_matmul(self.L, xt1) - xt0  # M x N\n",
    "                xt = concat(xt, xt2)\n",
    "                xt0, xt1 = xt1, xt2\n",
    "            xt = tf.transpose(xt)  # N x M x K\n",
    "            xt = tf.reshape(xt, [-1,self.K])  # NM x K\n",
    "            # Filter\n",
    "            W = self._weight_variable([self.K, self.F])\n",
    "            y = tf.matmul(xt, W)  # NM x F\n",
    "            y = tf.reshape(y, [-1, M, self.F])  # N x M x F\n",
    "            # Bias and non-linearity\n",
    "#            b = self._bias_variable([1, 1, self.F])\n",
    "            b = self._bias_variable([1, M, self.F])\n",
    "            y += b  # N x M x F\n",
    "            y = tf.nn.relu(y)\n",
    "        with tf.name_scope('fc1'):\n",
    "            W = self._weight_variable([self.F*M, NCLASSES])\n",
    "            b = self._bias_variable([NCLASSES])\n",
    "            y = tf.reshape(y, [-1, self.F*M])\n",
    "            y = tf.matmul(y, W) + b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cgcnn(base_model):\n",
    "    \"\"\"\n",
    "    Graph CNN which uses the Chebyshev approximation.\n",
    "\n",
    "    The following are hyper-parameters of graph convolutional layers.\n",
    "    They are lists, which length is equal to the number of gconv layers.\n",
    "        F: Number of features.\n",
    "        K: List of polynomial orders, i.e. filter sizes or number of hopes.\n",
    "        p: Pooling size.\n",
    "           Should be 1 (no pooling) or a power of 2 (reduction by 2 at each coarser level).\n",
    "           Beware to have coarsened enough.\n",
    "\n",
    "    L: List of Graph Laplacians. Size M x M. One per coarsening level.\n",
    "\n",
    "    The following are hyper-parameters of fully connected layers.\n",
    "    They are lists, which length is equal to the number of fc layers.\n",
    "        M: Number of features per sample, i.e. number of hidden neurons.\n",
    "    \n",
    "    The following are choices of implementation for various blocks.\n",
    "        filter: filtering operation, e.g. chebyshev5, lanczos2 etc.\n",
    "        brelu: bias and relu, e.g. b1relu or b2relu.\n",
    "        pool: pooling, e.g. mpool1.\n",
    "    \"\"\"\n",
    "    def __init__(self, L, F, K, p, M, filter='chebyshev5', brelu='b1relu', pool='mpool1'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Verify the consistency w.r.t. the number of layers.\n",
    "        assert len(L) >= len(F) == len(K) == len(p)\n",
    "        assert np.all(np.array(p) >= 1)\n",
    "        p_log2 = np.where(np.array(p) > 1, np.log2(p), 0)\n",
    "        assert np.all(np.mod(p_log2, 1) == 0)  # Powers of 2.\n",
    "        assert len(L) >= 1 + np.sum(p_log2)  # Enough coarsening levels for pool sizes.\n",
    "        \n",
    "        # Keep the useful Laplacians only.\n",
    "        j = 0\n",
    "        self.L = []\n",
    "        L_shape = []\n",
    "        for i in range(len(p)):\n",
    "            self.L.append(L[j])\n",
    "            L_shape.append(L[j].shape[0])\n",
    "            j += int(np.log2(p[i])) if p[i] > 1 else 0\n",
    "        L = self.L\n",
    "        for i in range(len(L)):\n",
    "            L[i] = graph.rescale_L(L[i], lmax=2)\n",
    "        \n",
    "        # Transform the Laplacians to TF sparse matrices.\n",
    "        if filter == 'chebyshev5':\n",
    "            with tf.name_scope('laplacians'):\n",
    "                for i in range(len(L)):\n",
    "                    L[i] = L[i].tocoo()\n",
    "                    data = L[i].data\n",
    "                    indices = np.empty((L[i].nnz, 2))\n",
    "                    indices[:,0] = L[i].row\n",
    "                    indices[:,1] = L[i].col\n",
    "                    L[i] = tf.SparseTensor(indices, data, L[i].shape)\n",
    "                    L[i] = tf.sparse_reorder(L[i])\n",
    "        \n",
    "        # Store attributes and bind operations.\n",
    "        self.L, self.F, self.K, self.p, self.M = L, F, K, p, M\n",
    "        self.filter = getattr(self, filter)\n",
    "        self.brelu = getattr(self, brelu)\n",
    "        self.pool = getattr(self, pool)\n",
    "        \n",
    "        # Print information about NN architecture.\n",
    "        Ngconv = len(p)\n",
    "        Nfc = len(M)\n",
    "        print('NN architecture')\n",
    "        print('  input: M_0 = {}'.format(L_shape[0]))\n",
    "        for i in range(Ngconv):\n",
    "            print('  layer {0}: cgconv{0}'.format(i+1))\n",
    "            print('    representation: M_{0} * F_{0} / p_{0} = {1} * {2} / {3} = {4}'.format(\n",
    "                    i+1, L_shape[i], F[i], p[i], L_shape[i]*F[i]//p[i]))\n",
    "            F_last = F[i-1] if i > 0 else 1\n",
    "            print('    weights: F_{1} * F_{0} * K_{0} = {2} * {3} * {4} = {5}'.format(\n",
    "                    i+1, i, F_last, F[i], K[i], F_last*F[i]*K[i]))\n",
    "            if brelu == 'b1relu':\n",
    "                print('    biases: F_{} = {}'.format(i+1, F[i]))\n",
    "            elif brelu == 'b2relu':\n",
    "                print('    biases: M_{0} * F_{0} = {1} * {2} = {3}'.format(\n",
    "                        i+1, L_shape[i], F[i], L_shape[i]*F[i]))\n",
    "        def M_last(i):\n",
    "            return M[i-1] if i > 0 else L_shape[-1] * F[-1] // p[-1]\n",
    "        def lprint(i, M_i):\n",
    "            print('    representation: M_{} = {}'.format(Ngconv+i+1, M_i))\n",
    "            print('    weights: M_{} * M_{} = {} * {} = {}'.format(\n",
    "                    Ngconv+i, Ngconv+i+1, M_last(i), M_i, M_last(i)*M_i))\n",
    "            print('    biases: M_{} = {}'.format(Ngconv+i+1, M_i))\n",
    "        for i in range(Nfc):\n",
    "            print('  layer {}: fc{}'.format(Ngconv+i+1, i+1))\n",
    "            lprint(i, M[i])\n",
    "        print('  layer {}: logits (softmax)'.format(Ngconv+Nfc+1))\n",
    "        lprint(Nfc, NCLASSES)\n",
    "\n",
    "    def chebyshev2(self, x, L, Fout, K):\n",
    "        \"\"\"\n",
    "        Filtering with Chebyshev interpolation\n",
    "        Implementation: numpy.\n",
    "        \n",
    "        Data: x of size N x M x F\n",
    "            N: number of signals\n",
    "            M: number of vertices\n",
    "            F: number of features per signal per vertex\n",
    "        \"\"\"\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Transform to Chebyshev basis\n",
    "        x = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
    "        x = tf.reshape(x, [M, Fin*N])  # M x Fin*N\n",
    "        def chebyshev(x):\n",
    "            return graph.chebyshev(L, x, K)\n",
    "        x = tf.py_func(chebyshev, [x], [tf.float32])[0]  # K x M x Fin*N\n",
    "        x = tf.reshape(x, [K, M, Fin, N])  # K x M x Fin x N\n",
    "        x = tf.transpose(x, perm=[3,1,2,0])  # N x M x Fin x K\n",
    "        x = tf.reshape(x, [N*M, Fin*K])  # N*M x Fin*K\n",
    "        # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature.\n",
    "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "        x = tf.matmul(x, W)  # NM x Fout\n",
    "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def chebyshev5(self, x, L, Fout, K):\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin)\n",
    "        # Transform to Chebyshev basis\n",
    "        x0 = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N\n",
    "        x0 = tf.reshape(x0, [M, Fin*N])  # M x Fin*N\n",
    "        x = tf.expand_dims(x0, 0)  # 1 x M x Fin*N\n",
    "        def concat(x, x_):\n",
    "            x_ = tf.expand_dims(x_, 0)  # 1 x M x Fin*N\n",
    "            return tf.concat(0, [x, x_])  # K x M x Fin*N\n",
    "        if K > 1:\n",
    "            x1 = tf.sparse_tensor_dense_matmul(L, x0)\n",
    "            x = concat(x, x1)\n",
    "        for k in range(2, K):\n",
    "            x2 = 2 * tf.sparse_tensor_dense_matmul(L, x1) - x0  # M x Fin*N\n",
    "            x = concat(x, x2)\n",
    "            x0, x1 = x1, x2\n",
    "        x = tf.reshape(x, [K, M, Fin, N])  # K x M x Fin x N\n",
    "        x = tf.transpose(x, perm=[3,1,2,0])  # N x M x Fin x K\n",
    "        x = tf.reshape(x, [N*M, Fin*K])  # N*M x Fin*K\n",
    "        # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature.\n",
    "        W = self._weight_variable([Fin*K, Fout], regularization=False)\n",
    "        x = tf.matmul(x, W)  # NM x Fout\n",
    "        return tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "\n",
    "    def b1relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def b2relu(self, x):\n",
    "        \"\"\"Bias and ReLU. One bias per vertex per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, int(M), int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b)\n",
    "\n",
    "    def mpool1(self, x, p):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            #tf.maximum\n",
    "            return tf.squeeze(x, [3])  # N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def fc(self, x, Mout, relu=True):\n",
    "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
    "        N, Min = x.get_shape()\n",
    "        W = self._weight_variable([int(Min), Mout], regularization=True)\n",
    "        b = self._bias_variable([Mout], regularization=True)\n",
    "        x = tf.matmul(x, W) + b\n",
    "        return tf.nn.relu(x) if relu else x\n",
    "\n",
    "    def _inference(self, x, dropout):\n",
    "        # Graph convolutional layers.\n",
    "        x = tf.expand_dims(x, 2)  # N x M x F=1\n",
    "        for i in range(len(self.p)):\n",
    "            with tf.variable_scope('cgconv{}'.format(i+1)):\n",
    "                with tf.name_scope('filter'):\n",
    "                    x = self.filter(x, self.L[i], self.F[i], self.K[i])\n",
    "                with tf.name_scope('bias_relu'):\n",
    "                    x = self.brelu(x)\n",
    "                with tf.name_scope('pooling'):\n",
    "                    x = self.pool(x, self.p[i])\n",
    "        \n",
    "        # Fully connected hidden layers.\n",
    "        N, M, F = x.get_shape()\n",
    "        x = tf.reshape(x, [int(N), int(M*F)])  # N x M\n",
    "        for i in range(len(self.M)):\n",
    "            with tf.variable_scope('fc{}'.format(i+1)):\n",
    "                x = self.fc(x, self.M[i])\n",
    "                x = tf.nn.dropout(x, dropout)\n",
    "        \n",
    "        # Logits linear layer, i.e. softmax without normalization.\n",
    "        with tf.variable_scope('logits'):\n",
    "            x = self.fc(x, NCLASSES, relu=False)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
