{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import graph\n",
    "import coarsening\n",
    "\n",
    "#import models\n",
    "%run -n models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Learning.\n",
    "flags.DEFINE_float('num_epochs', 10, 'Number of training epochs.')\n",
    "# 0.1 for cnn2, 0.3 for fgcnn2, 0.2 for lgcnn2\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_float('decay_rate', 0.95, 'Base of exponential decay. No decay with 1.')\n",
    "flags.DEFINE_float('momentum', 0.9, 'Momentum. 1 indicates no momentum.')\n",
    "\n",
    "# Regularizations.\n",
    "flags.DEFINE_float('regularization', 5e-4, 'L2 regularizations of weights and biases.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout regularization (fc layers): probability to keep hidden neurons.'\n",
    "                  'Deactivate with 1.')\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 100, 'Batch size. Must divide evenly into the dataset sizes.')\n",
    "flags.DEFINE_integer('eval_frequency', 300, 'Number of steps between evaluations.')\n",
    "\n",
    "# Graphs.\n",
    "flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n",
    "flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n",
    "flags.DEFINE_integer('coarsening_levels', 4, 'Number of coarsened graphs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grid_graph(m, corners=False):\n",
    "    z = graph.grid(m)\n",
    "    A = graph.adjacency(z, k=FLAGS.number_edges)\n",
    "\n",
    "    # Connections are only vertical or horizontal on the grid.\n",
    "    # Corner vertices are connected to 2 neightbors only.\n",
    "    if corners:\n",
    "        import scipy.sparse\n",
    "        A = A.toarray()\n",
    "        A[A < A.max()/1.5] = 0\n",
    "        A = scipy.sparse.csr_matrix(A)\n",
    "        print('{} edges'.format(A.nnz))\n",
    "\n",
    "    print(\"{} > {} edges\".format(A.nnz, FLAGS.number_edges*m**2))\n",
    "    return A\n",
    "\n",
    "def coarsen(A, levels):\n",
    "    graphs, parents = coarsening.metis(A, levels)\n",
    "    perms = coarsening.compute_perm(parents)\n",
    "\n",
    "    laplacians = []\n",
    "    for i,A in enumerate(graphs):\n",
    "        M, M = A.shape\n",
    "\n",
    "        # No self-connections.\n",
    "        if True:\n",
    "            A = A.tocoo()\n",
    "            A.setdiag(0)\n",
    "\n",
    "        if i < levels:\n",
    "            A = coarsening.perm_adjacency(A, perms[i])\n",
    "\n",
    "        A = A.tocsr()\n",
    "        A.eliminate_zeros()\n",
    "        Mnew, Mnew = A.shape\n",
    "        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz))\n",
    "\n",
    "        L = graph.laplacian(A, normalized=FLAGS.normalized_laplacian)\n",
    "        laplacians.append(L)\n",
    "    return laplacians, perms[0] if len(perms) > 0 else None\n",
    "\n",
    "t_start = time.process_time()\n",
    "A = grid_graph(28, corners=False)\n",
    "L, perm = coarsen(A, FLAGS.coarsening_levels)\n",
    "print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "del A\n",
    "\n",
    "if False:\n",
    "    for i,lap in enumerate(L):\n",
    "        lamb, U = graph.fourier(lap)\n",
    "        print('L_{}: spectrum in [{:1.2e}, {:1.2e}]'.format(i, lamb[0], lamb[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data_mnist\", one_hot=False)\n",
    "\n",
    "t_start = time.process_time()\n",
    "mnist.train._images = coarsening.perm_data(mnist.train._images, perm)\n",
    "mnist.validation._images = coarsening.perm_data(mnist.validation._images, perm)\n",
    "mnist.test._images = coarsening.perm_data(mnist.test._images, perm)\n",
    "print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "del perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, op_ncorrects, op_loss, data, labels):\n",
    "    \"\"\"\n",
    "    Runs one evaluation against the full epoch of data.\n",
    "    Return the precision and the number of correct predictions.\n",
    "    Batch evaluation saves memory and enables this to run on smaller GPUs.\n",
    "    \n",
    "    sess: the session in which the model has been trained.\n",
    "    op: the Tensor that returns the number of correct predictions.\n",
    "    data: size N x M\n",
    "        N: number of signals (samples)\n",
    "        M: number of vertices (features)\n",
    "    labels: size N\n",
    "        N: number of signals (samples)\n",
    "    \"\"\"\n",
    "    ncorrects = 0  # Counts the number of correct predictions.\n",
    "    loss = 0\n",
    "    size = data.shape[0]\n",
    "    for begin in range(0, size, FLAGS.batch_size):\n",
    "        end = begin + FLAGS.batch_size\n",
    "        batch_data, batch_labels = data[begin:end,:], labels[begin:end]\n",
    "        feed_dict = {ph_data: batch_data, ph_labels: batch_labels, 'dropout:0': 1}\n",
    "        batch_ncorrects, batch_loss = sess.run([op_ncorrects, op_loss], feed_dict)\n",
    "        ncorrects += batch_ncorrects\n",
    "        loss += batch_loss\n",
    "    precision = ncorrects / size\n",
    "    loss /= size\n",
    "    return 'precision: {:.2f}% ({:d} / {:d}), loss: {:.2e}'.format(precision*100, ncorrects, size, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = fc1()\n",
    "model = fc2(nhiddens=100)\n",
    "model = cnn2(K=5, F=10)  # K=28 is equivalent to filtering with fgcnn.\n",
    "model = fcnn2(F=10)\n",
    "model = fgcnn2(L[0], F=10)\n",
    "model = lgcnn2_2(L[0], F=10, K=10)\n",
    "model = cgcnn(L, F=[10], K=[20], p=[1], M=[])  # --> cgcnn2_2(L[0], F=10, K=20)\n",
    "#model = cgcnn2_3(L[0], F=10, K=5)\n",
    "#model = cgcnn2_4(L[0], F=10, K=5)\n",
    "#model = cgcnn2_5(L[0], F=10, K=5)\n",
    "\n",
    "# Architecture of TF MNIST conv model (LeNet-5-like).\n",
    "# Changes: regularization, dropout, decaying training rate, momentum optimizer, stopping condition.\n",
    "# Differences: training data randomization, init conv1 biases at 0.\n",
    "model = cgcnn(L, F=[32,64], K=[25,25], p=[4,4], M=[512])\n",
    "\n",
    "if False:\n",
    "    K = 5  # 5 or 5^2\n",
    "    t_start = time.process_time()\n",
    "    mnist.test._images = graph.lanczos(L, mnist.test._images.T, K).T\n",
    "    mnist.train._images = graph.lanczos(L, mnist.train._images.T, K).T\n",
    "    model = lgcnn2_1(L, F=10, K=K)\n",
    "    print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "    ph_data = tf.placeholder(tf.float32, (FLAGS.batch_size, mnist.train.images.shape[1], K), 'data')\n",
    "else:\n",
    "    ph_data = tf.placeholder(tf.float32, (FLAGS.batch_size, mnist.train.images.shape[1]), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ph_labels = tf.placeholder(tf.int32, (FLAGS.batch_size), 'labels')\n",
    "\n",
    "# Construct computational graph.\n",
    "op_logits = model.inference(ph_data)\n",
    "op_loss = model.loss(op_logits, ph_labels, FLAGS.regularization)\n",
    "op_train, op_learning_rate = model.training(op_loss, FLAGS.learning_rate,\n",
    "        mnist.train.num_examples/FLAGS.batch_size, FLAGS.decay_rate, FLAGS.momentum)\n",
    "op_ncorrects = model.evaluation(op_logits, ph_labels)\n",
    "\n",
    "# Initialize variables, i.e. weights and biases.\n",
    "t_start = time.process_time()\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Training.\n",
    "num_steps = int(FLAGS.num_epochs * mnist.train.num_examples / FLAGS.batch_size)\n",
    "for step in range(1, num_steps+1):\n",
    "    batch_data, batch_labels = mnist.train.next_batch(FLAGS.batch_size)\n",
    "    feed_dict = {ph_data: batch_data, ph_labels: batch_labels, 'dropout:0': FLAGS.dropout}\n",
    "    learning_rate, _ = sess.run([op_learning_rate, op_train], feed_dict)\n",
    "    \n",
    "    # Periodical evaluation of the model.\n",
    "    if step % FLAGS.eval_frequency == 0 or step == num_steps:\n",
    "        print('step {} / {} (epoch {:.2f} / {}), learning_rate={:.2e}:'.format(\n",
    "                step, num_steps, step*FLAGS.batch_size/mnist.train.num_examples, FLAGS.num_epochs, learning_rate))\n",
    "        print('  minibatch {}'.format(evaluate(sess, op_ncorrects, op_loss, batch_data, batch_labels)))\n",
    "        print('  validation {}'.format(evaluate(\n",
    "                sess, op_ncorrects, op_loss,mnist.validation.images, mnist.validation.labels)))\n",
    "        print('  time: {:.0f}s'.format(time.process_time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate.\n",
    "t_start = time.process_time()\n",
    "print('test {}'.format(evaluate(sess, op_ncorrects, op_loss, mnist.test.images, mnist.test.labels)))\n",
    "print('time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "#sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
