{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn, sklearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import time, re\n",
    "import graph, coarsening\n",
    "\n",
    "# TODO: model NFEATURES NCLASSES\n",
    "#import models\n",
    "%run -n models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Learning.\n",
    "flags.DEFINE_float('num_epochs', 10, 'Number of training epochs.')\n",
    "# 0.1 for cnn2, 0.3 for fgcnn2, 0.2 for lgcnn2\n",
    "flags.DEFINE_float('learning_rate', 0.001, 'Initial learning rate.')\n",
    "flags.DEFINE_float('decay_rate', 1, 'Base of exponential decay. No decay with 1.')\n",
    "flags.DEFINE_float('momentum', 0, 'Momentum. 0 indicates no momentum.')\n",
    "\n",
    "# Regularizations.\n",
    "flags.DEFINE_float('regularization', 5e-4, 'L2 regularizations of weights and biases.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout regularization (fc layers): probability to keep hidden neurons.'\n",
    "                  'Deactivate with 1.')\n",
    "\n",
    "flags.DEFINE_integer('batch_size', 100, 'Batch size. Must divide evenly into the dataset sizes.')\n",
    "flags.DEFINE_integer('eval_frequency', 100, 'Number of steps between evaluations.')\n",
    "\n",
    "# Graphs.\n",
    "flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n",
    "flags.DEFINE_string('metric', 'cosine', 'Graph: similarity measure (between features).')\n",
    "# TODO: change cgcnn for combinatorial Laplacians.\n",
    "flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n",
    "flags.DEFINE_integer('coarsening_levels', 0, 'Number of coarsened graphs.')\n",
    "\n",
    "# Directories.\n",
    "flags.DEFINE_string('dir_data', 'data_20news', 'Directory to store data.')\n",
    "flags.DEFINE_string('dir_summaries', 'summaries/mnist1/run1', 'Directory for TensorBoard summaries.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def fetch(self, **params):\n",
    "        dataset = sklearn.datasets.fetch_20newsgroups(**params)\n",
    "        self.documents = dataset.data\n",
    "        self.labels = dataset.target\n",
    "        self.class_names = dataset.target_names\n",
    "        assert max(self.labels) + 1 == len(self.class_names)\n",
    "        N, C = len(self.documents), len(self.class_names)\n",
    "        print('N = {} documents, C = {} classes'.format(N, C))\n",
    "        #print(train.target_names)\n",
    "\n",
    "    def clean_text(self, num='substitute'):\n",
    "        # TODO: stemming, lemmatisation\n",
    "        for i,doc in enumerate(self.documents):\n",
    "            # Digits.\n",
    "            if num is 'spell':\n",
    "                doc = doc.replace('0', ' zero ')\n",
    "                doc = doc.replace('1', ' one ')\n",
    "                doc = doc.replace('2', ' two ')\n",
    "                doc = doc.replace('3', ' three ')\n",
    "                doc = doc.replace('4', ' four ')\n",
    "                doc = doc.replace('5', ' five ')\n",
    "                doc = doc.replace('6', ' six ')\n",
    "                doc = doc.replace('7', ' seven ')\n",
    "                doc = doc.replace('8', ' eight ')\n",
    "                doc = doc.replace('9', ' nine ')\n",
    "            elif num is 'substitute':\n",
    "                # All numbers are equal. Useful for embedding (countable words) ?\n",
    "                doc = re.sub('(\\\\d+)', ' NUM ', doc)\n",
    "            elif num is 'remove':\n",
    "                # Numbers are uninformative (they are all over the place). Useful for bag-of-words ?\n",
    "                # But maybe some kind of documents contain more numbers, e.g. finance.\n",
    "                # Some documents are indeed full of numbers. At least in 20NEWS.\n",
    "                doc = re.sub('[0-9]', ' ', doc)\n",
    "            # Remove everything except a-z characters and single space.\n",
    "            doc = doc.replace('$', ' dollar ')\n",
    "            doc = doc.lower()\n",
    "            doc = re.sub('[^a-z]', ' ', doc)\n",
    "            doc = ' '.join(doc.split())  # same as doc = re.sub('\\s{2,}', ' ', doc)\n",
    "            self.documents[i] = doc\n",
    "\n",
    "    def vectorize(self, **params):\n",
    "        # TODO: count or tf-idf. Or in normalize ?\n",
    "        vectorizer = sklearn.feature_extraction.text.CountVectorizer(**params)\n",
    "        self.data = vectorizer.fit_transform(self.documents)\n",
    "        self.vocab = vectorizer.get_feature_names()\n",
    "        assert len(self.vocab) == self.data.shape[1]\n",
    "    \n",
    "    def data_info(self):\n",
    "        N, M = self.data.shape\n",
    "        sparsity = self.data.nnz / N / M * 100\n",
    "        print('N = {} documents, M = {} words, sparsity={:.4f}%'.format(N, M, sparsity))\n",
    "        \n",
    "    def show_document(self, i):\n",
    "        label = self.labels[i]\n",
    "        name = self.class_names[label]\n",
    "        text = self.documents[i]\n",
    "        wc = len(text.split())\n",
    "        print('document {}: label {} --> {}, {} words'.format(i, label, name, wc))\n",
    "        try:\n",
    "            vector = self.data[i,:]\n",
    "            for j in range(vector.shape[1]):\n",
    "                if vector[0,j] != 0:\n",
    "                    print('  {:.2f} \"{}\" ({})'.format(vector[0,j], self.vocab[j], j))\n",
    "        except:\n",
    "            pass\n",
    "        return text\n",
    "    \n",
    "    def keep_documents(self, idx):\n",
    "        \"\"\"Keep the documents given by the index, discard the others.\"\"\"\n",
    "        self.documents = [self.documents[i] for i in idx]\n",
    "        self.labels = self.labels[idx]\n",
    "        self.data = self.data[idx,:]\n",
    "\n",
    "    def keep_words(self, idx):\n",
    "        \"\"\"Keep the documents given by the index, discard the others.\"\"\"\n",
    "        self.data = self.data[:,idx]\n",
    "        self.vocab = [self.vocab[i] for i in idx]\n",
    "        \n",
    "    def remove_short_documents(self, nwords, vocab='selected'):\n",
    "        \"\"\"Remove a document if it contains less than nwords.\"\"\"\n",
    "        if vocab is 'selected':\n",
    "            # Word count with selected vocabulary.\n",
    "            wc = self.data.sum(axis=1)\n",
    "            wc = np.squeeze(np.asarray(wc))\n",
    "        elif vocab is 'full':\n",
    "            # Word count with full vocabulary.\n",
    "            wc = np.empty(len(self.documents), dtype=np.int)\n",
    "            for i,doc in enumerate(self.documents):\n",
    "                wc[i] = len(doc.split())\n",
    "        idx = np.argwhere(wc >= nwords).squeeze()\n",
    "        self.keep_documents(idx)\n",
    "        return wc\n",
    "        \n",
    "    def keep_top_words(self, M, Mprint=20):\n",
    "        \"\"\"Keep in the vocaluary the M words who appear most often.\"\"\"\n",
    "        freq = self.data.sum(axis=0)\n",
    "        freq = np.squeeze(np.asarray(freq))\n",
    "        idx = np.argsort(freq)[::-1]\n",
    "        idx = idx[:M]\n",
    "        self.keep_words(idx)\n",
    "        print('most frequent words')\n",
    "        for i in range(Mprint):\n",
    "            print('  {:3d}: {:10s} {:6d} counts'.format(i, self.vocab[i], freq[idx][i]))\n",
    "        return freq[idx]\n",
    "    \n",
    "    def normalize(self, norm='l1'):\n",
    "        \"\"\"Normalize data to unit length.\"\"\"\n",
    "        # TODO: TF-IDF.\n",
    "        data = self.data.astype(np.float64)\n",
    "        self.data = sklearn.preprocessing.normalize(data, axis=1, norm=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fetch dataset. Scikit-learn already performs some cleaning.\n",
    "remove = ('headers','footers','quotes')  # (), ('headers') or ('headers','footers','quotes')\n",
    "train = Dataset()\n",
    "train.fetch(data_home=FLAGS.dir_data, subset='train', remove=remove)\n",
    "\n",
    "# Pre-processing: transform everything to a-z and whitespace.\n",
    "print(train.show_document(1)[:400])\n",
    "train.clean_text(num='substitute')\n",
    "\n",
    "# Analyzing / tokenizing: transform documents to bags-of-words.\n",
    "#stop_words = set(sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)\n",
    "# Or stop words from NLTK.\n",
    "# Add e.g. don, ve.\n",
    "train.vectorize(stop_words='english')\n",
    "print(train.show_document(1)[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove short documents.\n",
    "train.data_info()\n",
    "wc = train.remove_short_documents(nwords=20, vocab='full')\n",
    "train.data_info()\n",
    "print('shortest: {}, longest: {} words'.format(wc.min(), wc.max()))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.semilogy(wc, '.');\n",
    "\n",
    "# Remove encoded images.\n",
    "def remove_encoded_images(dataset, freq=1e3):\n",
    "    widx = train.vocab.index('ax')\n",
    "    wc = train.data[:,widx].toarray().squeeze()\n",
    "    idx = np.argwhere(wc < freq).squeeze()\n",
    "    dataset.keep_documents(idx)\n",
    "    return wc\n",
    "wc = remove_encoded_images(train)\n",
    "train.data_info()\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.semilogy(wc, '.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature selection.\n",
    "freq = train.keep_top_words(1000, 20)\n",
    "train.data_info()\n",
    "train.show_document(1)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.semilogy(freq);\n",
    "\n",
    "# Remove documents whose signal would be the zero vector.\n",
    "wc = train.remove_short_documents(nwords=5, vocab='selected')\n",
    "train.data_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.normalize(norm='l1')\n",
    "train.show_document(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Word embedding\n",
    "# Further feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test dataset.\n",
    "test = Dataset()\n",
    "test.fetch(data_home=FLAGS.dir_data, subset='test', remove=remove)\n",
    "test.clean_text(num='substitute')\n",
    "test.vectorize(vocabulary=train.vocab)\n",
    "test.data_info()\n",
    "wc = test.remove_short_documents(nwords=5, vocab='selected')\n",
    "print('shortest: {}, longest: {} words'.format(wc.min(), wc.max()))\n",
    "test.data_info()\n",
    "test.normalize(norm='l1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coarsen(A, levels):\n",
    "    graphs, parents = coarsening.metis(A, levels)\n",
    "    perms = coarsening.compute_perm(parents)\n",
    "\n",
    "    laplacians = []\n",
    "    for i,A in enumerate(graphs):\n",
    "        M, M = A.shape\n",
    "\n",
    "        # No self-connections.\n",
    "        if True:\n",
    "            A = A.tocoo()\n",
    "            A.setdiag(0)\n",
    "\n",
    "        if i < levels:\n",
    "            A = coarsening.perm_adjacency(A, perms[i])\n",
    "\n",
    "        A = A.tocsr()\n",
    "        A.eliminate_zeros()\n",
    "        Mnew, Mnew = A.shape\n",
    "        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz))\n",
    "\n",
    "        L = graph.laplacian(A, normalized=FLAGS.normalized_laplacian)\n",
    "        laplacians.append(L)\n",
    "    return laplacians, perms[0] if len(perms) > 0 else None\n",
    "\n",
    "t_start = time.process_time()\n",
    "A = graph.adjacency(train.data.T.toarray(), k=FLAGS.number_edges, metric=FLAGS.metric)\n",
    "print(\"{} > {} edges\".format(A.nnz, FLAGS.number_edges*train.data.shape[1]))\n",
    "L, perm = coarsen(A, FLAGS.coarsening_levels)\n",
    "print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "del A\n",
    "\n",
    "if True:\n",
    "    for i,lap in enumerate(L):\n",
    "        lamb, U = graph.fourier(lap)\n",
    "        print('L_{}: spectrum in [{:1.2e}, {:1.2e}]'.format(i, lamb[0], lamb[-1]))\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(lamb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Nval = 200\n",
    "perm = np.random.permutation(train.data.shape[0])\n",
    "val_data = train.data[perm[:Nval]].toarray()\n",
    "val_labels = train.labels[perm[:Nval]]\n",
    "train_data = train.data[perm[Nval:]].toarray()\n",
    "train_labels = train.labels[perm[Nval:]]\n",
    "\n",
    "# TODO: not a multiple of minibatch size\n",
    "test_data, test_labels = test.data[:800].toarray(), test.labels[:800]\n",
    "\n",
    "# TODO: coarsening\n",
    "t_start = time.process_time()\n",
    "train_data = coarsening.perm_data(train_data, perm)\n",
    "val_data = coarsening.perm_data(val_data, perm)\n",
    "test_data = coarsening.perm_data(test_data, perm)\n",
    "print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "del perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, ph_dropout, op_ncorrects, op_loss, data, labels):\n",
    "    \"\"\"\n",
    "    Runs one evaluation against the full epoch of data.\n",
    "    Return the precision and the number of correct predictions.\n",
    "    Batch evaluation saves memory and enables this to run on smaller GPUs.\n",
    "    \n",
    "    sess: the session in which the model has been trained.\n",
    "    op: the Tensor that returns the number of correct predictions.\n",
    "    data: size N x M\n",
    "        N: number of signals (samples)\n",
    "        M: number of vertices (features)\n",
    "    labels: size N\n",
    "        N: number of signals (samples)\n",
    "    \"\"\"\n",
    "    ncorrects = 0  # Counts the number of correct predictions.\n",
    "    loss = 0\n",
    "    size = data.shape[0]\n",
    "    for begin in range(0, size, FLAGS.batch_size):\n",
    "        end = begin + FLAGS.batch_size\n",
    "        batch_data, batch_labels = data[begin:end,:], labels[begin:end]\n",
    "        feed_dict = {ph_data: batch_data, ph_labels: batch_labels, ph_dropout: 1}\n",
    "        batch_ncorrects, batch_loss = sess.run([op_ncorrects, op_loss], feed_dict)\n",
    "        ncorrects += batch_ncorrects\n",
    "        loss += batch_loss\n",
    "    precision = ncorrects / size * 100\n",
    "    loss *= FLAGS.batch_size / size\n",
    "    string = 'precision: {:.2f}% ({:d} / {:d}), loss: {:.2e}'.format(precision, ncorrects, size, loss)\n",
    "    return string, precision, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = cgcnn(L, F=[32], K=[25], p=[1], M=[512])\n",
    "\n",
    "Nval = 200\n",
    "perm = np.random.permutation(train.data.shape[0])\n",
    "val_data = train.data[perm[:Nval]].toarray()\n",
    "val_labels = train.labels[perm[:Nval]]\n",
    "train_data = train.data[perm[Nval:]].toarray()\n",
    "train_labels = train.labels[perm[Nval:]]\n",
    "\n",
    "#train_(model, train_data, train_labels, val_data, val_labels, FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def train_(model, train_data, train_labels, val_data, val_labels, FLAGS):\n",
    "with tf.name_scope('inputs'):\n",
    "    ph_data = tf.placeholder(tf.float32, (FLAGS.batch_size, train_data.shape[1]), 'data')\n",
    "    ph_labels = tf.placeholder(tf.int32, (FLAGS.batch_size), 'labels')\n",
    "    ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
    "\n",
    "# Construct computational graph.\n",
    "op_logits = model.inference(ph_data, ph_dropout)\n",
    "op_loss, op_loss_average = model.loss(op_logits, ph_labels, FLAGS.regularization)\n",
    "op_train = model.training(op_loss, FLAGS.learning_rate,\n",
    "        train_data.shape[0]/FLAGS.batch_size, FLAGS.decay_rate, FLAGS.momentum)\n",
    "op_ncorrects = model.evaluation(op_logits, ph_labels)\n",
    "\n",
    "# Summaries for TensorBoard.\n",
    "op_summary = tf.merge_all_summaries()\n",
    "sess = tf.Session()\n",
    "writer = tf.train.SummaryWriter(FLAGS.dir_summaries, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize variables, i.e. weights and biases.\n",
    "t_start = time.process_time()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# Training.\n",
    "indices = []\n",
    "num_steps = int(FLAGS.num_epochs * train_data.shape[0] / FLAGS.batch_size)\n",
    "for step in range(1, num_steps+1):\n",
    "\n",
    "    # Be sure to have used all the samples before using one a second time.\n",
    "    # TODO: queue\n",
    "    if len(indices) < FLAGS.batch_size:\n",
    "        new_indices = np.random.permutation(train_data.shape[0])\n",
    "        indices.extend(new_indices)\n",
    "    idx = indices[:FLAGS.batch_size]\n",
    "    del indices[:FLAGS.batch_size]\n",
    "\n",
    "    batch_data, batch_labels = train_data[idx,:], train_labels[idx]\n",
    "    feed_dict = {ph_data: batch_data, ph_labels: batch_labels, ph_dropout: FLAGS.dropout}\n",
    "    learning_rate, loss_average = sess.run([op_train, op_loss_average], feed_dict)\n",
    "\n",
    "    # Periodical evaluation of the model.\n",
    "    if step % FLAGS.eval_frequency == 0 or step == num_steps:\n",
    "        epoch = step * FLAGS.batch_size / train_data.shape[0]\n",
    "        print('step {} / {} (epoch {:.2f} / {}):'.format(step, num_steps, epoch, FLAGS.num_epochs))\n",
    "        print('  learning_rate = {:.2e}, loss_average = {:.2e}'.format(learning_rate, loss_average))\n",
    "        string, precision, loss = evaluate(sess, ph_dropout, op_ncorrects, op_loss, val_data, val_labels) \n",
    "        print('  validation {}'.format(string))\n",
    "        print('  time: {:.0f}s'.format(time.process_time() - t_start))\n",
    "\n",
    "        # Summaries for TensorBoard.\n",
    "        summary = tf.Summary()\n",
    "        summary.ParseFromString(sess.run(op_summary, feed_dict))\n",
    "        summary.value.add(tag='validation/precision', simple_value=precision)\n",
    "        summary.value.add(tag='validation/loss', simple_value=loss)\n",
    "        writer.add_summary(summary, step)\n",
    "writer.close()\n",
    "\n",
    "#return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate.\n",
    "t_start = time.process_time()\n",
    "print('test {}'.format(evaluate(sess, ph_dropout, op_ncorrects, op_loss, test_data, test_labels)[0]))\n",
    "print('time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "#sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
