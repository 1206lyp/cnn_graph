{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: classification with learned graph filters\n",
    "\n",
    "We want to classify data by first extracting meaningful features from learned filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse, scipy.sparse.linalg, scipy.spatial.distance\n",
    "from sklearn import datasets, linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "* Two digits version of MNIST with N samples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mnist(a, b, N):\n",
    "    \"\"\"Prepare data for binary classification of MNIST.\"\"\"\n",
    "    mnist = datasets.fetch_mldata('MNIST original', data_home='.')\n",
    "\n",
    "    assert N < min(sum(mnist.target==a), sum(mnist.target==b))\n",
    "    M = mnist.data.shape[1]\n",
    "    \n",
    "    X = np.empty((M, 2, N))\n",
    "    X[:,0,:] = mnist.data[mnist.target==a,:][:N,:].T\n",
    "    X[:,1,:] = mnist.data[mnist.target==b,:][:N,:].T\n",
    "    \n",
    "    y = np.empty((2, N))\n",
    "    y[0,:] = -1\n",
    "    y[1,:] = +1\n",
    "\n",
    "    X.shape = M, 2*N\n",
    "    y.shape = 2*N, 1\n",
    "    return X, y\n",
    "\n",
    "X, y = mnist(5, 1, 1000)\n",
    "\n",
    "M, N = X.shape\n",
    "print('Dimensionality: N={} samples, M={} features'.format(N, M))\n",
    "\n",
    "X -= 127.5\n",
    "print('X in [{}, {}]'.format(np.min(X), np.max(X)))\n",
    "\n",
    "def plot_digit(nn):\n",
    "    m = int(np.sqrt(M))\n",
    "    fig, axes = plt.subplots(1,len(nn), figsize=(15,5))\n",
    "    for i, n in enumerate(nn):\n",
    "        n = int(n)\n",
    "        img = X[:,n]\n",
    "        axes[i].imshow(img.reshape((m,m)))\n",
    "        axes[i].set_title('Label: y = {:.0f}'.format(y[n,0]))\n",
    "\n",
    "plot_digit([0, 1, 1e2, 1e2+1, 1e3, 1e3+1])\n",
    "\n",
    "#M, N = 784, 1000\n",
    "#X = X[:M, :N]\n",
    "#y = y[:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized least-square\n",
    "\n",
    "## Loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L(w, b=0):\n",
    "    return np.linalg.norm(X.T @ w + b - y)**2 / N + tauR * np.linalg.norm(w)**2\n",
    "\n",
    "def dL(w, nn=None):\n",
    "    N = len(y)\n",
    "    return 2 / N * X @ (X.T @ w - y) + 2 * tauR * w\n",
    "\n",
    "def print_perf(w, L, dL):\n",
    "    print('L({}) = {}'.format(w, L(eval(w))))\n",
    "    print('|dL({})| = {}'.format(w, np.linalg.norm(dL(eval(w)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: sklearn ridge regression\n",
    "\n",
    "* With regularized data, the objective is the same with or without bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tauR = 1e0\n",
    "\n",
    "clf = linear_model.Ridge(alpha=tauR*N, fit_intercept=False)\n",
    "clf.fit(X.T, y)\n",
    "w_skl = clf.coef_.T\n",
    "\n",
    "print('L(w_skl) = {}'.format(L(w_skl, clf.intercept_)))\n",
    "print_perf('w_skl', L, dL)\n",
    "\n",
    "# Normalized data: intercept should be small.\n",
    "print('bias: {}'.format(abs(np.mean(y - X.T @ w_skl))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_d = np.linalg.inv(X @ X.T + tauR * N * np.identity(M)) @ X @ y\n",
    "print_perf('w_d', L, dL)\n",
    "np.testing.assert_allclose(w_d, w_skl, atol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def graph_grid(k=4):\n",
    "    \"\"\"Construct a kNN graph aranged on a 2D grid.\"\"\"\n",
    "    \n",
    "    # Construct a grid.\n",
    "    m = np.int(np.sqrt(M))\n",
    "    x = np.linspace(0,1,m)\n",
    "    y = np.linspace(0,1,m)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = np.empty((M,2))\n",
    "    z[:,0] = xx.reshape(M)\n",
    "    z[:,1] = yy.reshape(M)\n",
    "\n",
    "    # Compute pairwise distances.\n",
    "    d = scipy.spatial.distance.pdist(z, 'euclidean')\n",
    "    d = scipy.spatial.distance.squareform(d)\n",
    "\n",
    "    # k-NN graph.\n",
    "    idx = np.argsort(d)[:,1:k+1]\n",
    "    d.sort()\n",
    "    d = d[:,1:k+1]\n",
    "\n",
    "    # Weights.\n",
    "    sigma2 = np.mean(d[:,-1])**2\n",
    "    d = np.exp(- d**2 / sigma2)\n",
    "\n",
    "    # Weight matrix.\n",
    "    I = np.arange(0, M).repeat(k)\n",
    "    J = idx.reshape(M*k)\n",
    "    V = d.reshape(M*k)\n",
    "    W = scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n",
    "    \n",
    "    W = graph_regularize(W)\n",
    "    print(\"{} > {} edges\".format(W.nnz, M*k))\n",
    "    return W\n",
    "\n",
    "def graph_regularize(W):\n",
    "    # No self-connections.\n",
    "    W.setdiag(0)\n",
    "\n",
    "    # Non-directed graph.\n",
    "    bigger = W.T > W\n",
    "    W = W - W.multiply(bigger) + W.T.multiply(bigger)\n",
    "    del bigger\n",
    "    assert np.abs(W - W.T).mean() < 1e-10\n",
    "\n",
    "    # CSR sparse matrix format for efficient multiplications.\n",
    "    W = W.tocsr()\n",
    "    W.eliminate_zeros()\n",
    "    \n",
    "    return W\n",
    "\n",
    "W = graph_grid(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def laplacian(W, normalized=True):\n",
    "    \"\"\"Return the Laplacian of the weigth matrix.\"\"\"\n",
    "    \n",
    "    # Degree matrix.\n",
    "    d = W.sum(axis=0)\n",
    "\n",
    "    # Laplacian matrix.\n",
    "    if not normalized:\n",
    "        D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "        L = D - W\n",
    "    else:\n",
    "        d = 1 / np.sqrt(d)\n",
    "        D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "        I = scipy.sparse.identity(M, dtype=D.dtype)\n",
    "        L = I - D * W * D\n",
    "    \n",
    "    # Upper-bound on the spectrum.\n",
    "    if normalized:\n",
    "        lmax = 2\n",
    "    else:\n",
    "        lmax = scipy.sparse.linalg.eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]\n",
    "    \n",
    "    assert np.abs(L - L.T).mean() < 1e-10\n",
    "    return L, lmax\n",
    "\n",
    "LL, lmax = laplacian(W, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanczos basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "\n",
    "def lanczos(L, X, K):\n",
    "    M, N = X.shape\n",
    "    a = np.empty((K, N))\n",
    "    b = np.zeros((K, N))\n",
    "    V = np.empty((K, M, N))\n",
    "    V[0,...] = X / np.linalg.norm(X, axis=0)\n",
    "    for k in range(K-1):\n",
    "        W = L.dot(V[k,...])\n",
    "        a[k,:] = np.sum(W * V[k,...], axis=0)\n",
    "        W = W - a[k,:] * V[k,...] - (b[k,:] * V[k-1,...] if k>0 else 0)\n",
    "        b[k+1,:] = np.linalg.norm(W, axis=0)\n",
    "        V[k+1,...] = W / b[k+1,:]\n",
    "    a[K-1,:] = np.sum(L.dot(V[K-1,...]) * V[K-1,...], axis=0)\n",
    "    return V, a, b\n",
    "\n",
    "def lanczos_H_diag(a, b):\n",
    "    K, N = a.shape\n",
    "    H = np.zeros((K*K, N))\n",
    "    H[:K**2:K+1, :] = a\n",
    "    H[1:(K-1)*K:K+1, :] = b[1:,:]\n",
    "    H.shape = (K, K, N)\n",
    "    Q = np.linalg.eigh(H.T, UPLO='L')[1]\n",
    "    Q = np.swapaxes(Q,1,2).T\n",
    "    return Q\n",
    "\n",
    "def lanczos_basis_eval(L, X, K):\n",
    "    V, a, b = lanczos(L, X, K)\n",
    "    Q = lanczos_H_diag(a, b)\n",
    "    M, N = X.shape\n",
    "    Xt = np.empty((K, M, N))\n",
    "    for n in range(N):\n",
    "        Xt[...,n] = Q[...,n].T @ V[...,n]\n",
    "    Xt *= Q[0,:,np.newaxis,:]\n",
    "    Xt *= np.linalg.norm(X, axis=0)\n",
    "    return Xt, Q[0,...]\n",
    "\n",
    "Xt, q = lanczos_basis_eval(LL, X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFL classification with weights\n",
    "\n",
    "* Memory arrangement for fastest computations: largest dimensions on the outside, i.e. fastest varying indices.\n",
    "* The einsum seems to be efficient for three operands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    \"\"\"Test the speed of filtering and weighting.\"\"\"\n",
    "    \n",
    "    def mult(impl=3):\n",
    "        if impl is 0:\n",
    "            Xb = Xt.view()\n",
    "            Xb.shape = (K, M*N)\n",
    "            XCb = Xb.T @ C  # in MN x F\n",
    "            XCb = XCb.T.reshape((F*M, N))\n",
    "            return (XCb.T @ w).squeeze()\n",
    "        elif impl is 1:\n",
    "            tmp = np.tensordot(Xt, C, (0,0))\n",
    "            return np.tensordot(tmp, W, ((0,2),(1,0)))\n",
    "        elif impl is 2:\n",
    "            tmp = np.tensordot(Xt, C, (0,0))\n",
    "            return np.einsum('ijk,ki->j', tmp, W)\n",
    "        elif impl is 3:\n",
    "            return np.einsum('kmn,fm,kf->n', Xt, W, C)\n",
    "    \n",
    "    C = np.random.normal(0,1,(K,F))\n",
    "    W = np.random.normal(0,1,(F,M))\n",
    "    w = W.reshape((F*M, 1))\n",
    "    a = mult(impl=0)\n",
    "    for impl in range(4):\n",
    "        tstart = time.process_time()\n",
    "        for k in range(1000):\n",
    "            b = mult(impl)\n",
    "        print('Execution time (impl={}): {}'.format(impl, time.process_time() - tstart))\n",
    "        np.testing.assert_allclose(a, b)\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class gflc_weights:\n",
    "\n",
    "def L(C, W):\n",
    "    tmp = np.einsum('kmn,kf,fm->n', Xt, C, W) - y.squeeze()\n",
    "    return np.linalg.norm(tmp)**2 / N + tauR * np.linalg.norm(W)**2\n",
    "\n",
    "def dLw(C, W):\n",
    "    tmp = np.einsum('kmn,kf,fm->n', Xt, C, W) - y.squeeze()\n",
    "    return 2 / N * np.einsum('kmn,kf,n->fm', Xt, C, tmp) + 2 * tauR * W\n",
    "    #return 2 / N * XCb @ (XCb.T @ w - y) + 2 * tauR * w\n",
    "\n",
    "def dLc(C, W):\n",
    "    tmp = np.einsum('kmn,kf,fm->n', Xt, C, W) - y.squeeze()\n",
    "    return 2 / N * np.einsum('kmn,n,fm->kf', Xt, tmp, W)\n",
    "\n",
    "def print_perf(C, W, loss):\n",
    "    print('L = {}'.format(L(C, W)))\n",
    "    print('|dLw| = {}'.format(np.linalg.norm(dLw(C, W))))\n",
    "    print('|dLc| = {}'.format(np.linalg.norm(dLc(C, W))))\n",
    "    plt.semilogy(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_optim(algo):\n",
    "    tstart = time.process_time()\n",
    "    ret = algo()\n",
    "    print('Processing time: {}'.format(time.process_time()-tstart))\n",
    "    print_perf(*ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd():\n",
    "    C = np.random.normal(0, 1, (K, F))\n",
    "    W = np.random.normal(0, 1, (F, M))\n",
    "    \n",
    "    loss = [L(C, W)]\n",
    "    \n",
    "    for t in range(100):\n",
    "        C -= 1e-8 * dLc(C, W)\n",
    "        W -= 1e-8 * dLw(C, W)\n",
    "        loss.append(L(C, W))\n",
    "        \n",
    "    return C, W, loss\n",
    "\n",
    "test_optim(sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def direct():\n",
    "    C = np.random.normal(0, 1, (K, F))\n",
    "    W = np.random.normal(0, 1, (F, M))\n",
    "    c = C.reshape((K*F, 1))\n",
    "    w = W.reshape((F*M, 1))\n",
    "\n",
    "    loss = [L(C, W)]\n",
    "\n",
    "    for t in range(5):\n",
    "        Xw = np.einsum('kmn,fm->kfn', Xt, W)\n",
    "        #Xw = np.tensordot(Xt, W, (1,1))\n",
    "        Xw.shape = (K*F, N)\n",
    "        c[:] = np.linalg.solve(Xw @ Xw.T, Xw @ y)\n",
    "\n",
    "        Z = np.einsum('kmn,kf->fmn', Xt, C)\n",
    "        #Z = np.tensordot(Xt, C, (0,0))\n",
    "        #Z = C.T @ Xt.reshape((K,M*N))\n",
    "        Z.shape = (F*M, N)\n",
    "        w[:] = np.linalg.solve(Z @ Z.T + tauR * N * np.identity(F*M), Z @ y)\n",
    "\n",
    "        loss.append(L(C, W))\n",
    "    return C, W, loss\n",
    "\n",
    "test_optim(direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GFL classification with splitting\n",
    "\n",
    "Solvers\n",
    "* Closed-form solution.\n",
    "* Stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tauF = 1e0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L(C, w, Z, XCb):\n",
    "    return np.linalg.norm(XCb.T @ w - y)**2 / N + tauR * np.linalg.norm(w)**2\n",
    "\n",
    "def Lsplit(C, w, Z, XCb):\n",
    "    return np.linalg.norm(Z.T @ w - y)**2 / N + tauF / N * np.linalg.norm(XCb - Z)**2 + tauR * np.linalg.norm(w)**2\n",
    "\n",
    "def dLw(C, w, Z, XCb):\n",
    "    return 2 / N * Z @ (Z.T @ w - y) + 2 * tauR * w\n",
    "\n",
    "def dLc(C, w, Z, XCb):\n",
    "    Xb = Xt.reshape((K, M*N)).T\n",
    "    Zb = Z.reshape((F, M*N)).T\n",
    "    return 2 * tauF / N * Xb.T @ (Xb @ C - Zb)\n",
    "\n",
    "def dLz(C, w, Z, XCb):\n",
    "    return 2 / N * w @ (w.T @ Z - y.T) + 2 * tauF / N * (Z - XCb)\n",
    "\n",
    "def print_perf(*args):\n",
    "    print('L = {}'.format(L(*args[:-2])))\n",
    "    print('Lsplit = {}'.format(Lsplit(*args[:-2])))\n",
    "    print('|dLw| = {}'.format(np.linalg.norm(dLw(*args[:-2]))))\n",
    "    print('|dLc| = {}'.format(np.linalg.norm(dLc(*args[:-2]))))\n",
    "    print('|dLz| = {}'.format(np.linalg.norm(dLz(*args[:-2]))))\n",
    "    plt.semilogy(args[-2])\n",
    "    #plt.figure()\n",
    "    plt.semilogy(args[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lanczos_filter(C):\n",
    "    Xb = Xt.reshape((K, M*N)).T\n",
    "    #XCb = np.tensordot(Xb, C, (2,1))\n",
    "    XCb = Xb @ C  # in MN x F\n",
    "    XCb = XCb.T.reshape((F*M, N))  # Needs to copy data.\n",
    "    return XCb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_split():\n",
    "    C = np.zeros((K, F))\n",
    "    w = np.zeros((F*M, 1))\n",
    "    Z = np.random.normal(0, 1, (F*M, N))\n",
    "    \n",
    "    XCb = np.empty((F*M, N))\n",
    "\n",
    "    loss = [L(C, w, Z, XCb)]\n",
    "    loss_split = [Lsplit(C, w, Z, XCb)]\n",
    "\n",
    "    for t in range(100):\n",
    "        C -= 1e-7 * dLc(C, w, Z, XCb)\n",
    "        XCb[:] = lanczos_filter(C)\n",
    "        Z -= 1e-3 * dLz(C, w, Z, XCb)\n",
    "        w -= 1e-3 * dLw(C, w, Z, XCb)\n",
    "        loss.append(L(C, w, Z, XCb))\n",
    "        loss_split.append(Lsplit(C, w, Z, XCb))\n",
    "        \n",
    "    return C, w, Z, XCb, loss, loss_split\n",
    "\n",
    "test_optim(sgd_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def direct_split():\n",
    "    C = np.zeros((K, F))\n",
    "    w = np.zeros((F*M, 1))\n",
    "    Z = np.random.normal(0, 1, (F*M, N))\n",
    "    \n",
    "    XCb = np.empty((F*M, N))\n",
    "    Xb = Xt.reshape((K, M*N)).T\n",
    "    Zb = Z.reshape((F, M*N)).T\n",
    "\n",
    "    loss = [L(C, w, Z, XCb)]\n",
    "    loss_split = [Lsplit(C, w, Z, XCb)]\n",
    "\n",
    "    for t in range(8):\n",
    "\n",
    "        C[:] = Xb.T @ Zb / np.sum((np.linalg.norm(X, axis=0) * q)**2, axis=1)[:,np.newaxis]\n",
    "        XCb[:] = lanczos_filter(C)\n",
    "\n",
    "        #Z[:] = np.linalg.inv(tauF * np.identity(F*M) + w @ w.T) @ (tauF * XCb + w @ y.T)\n",
    "        Z[:] = np.linalg.solve(tauF * np.identity(F*M) + w @ w.T, tauF * XCb + w @ y.T)\n",
    "\n",
    "        #w[:] = np.linalg.inv(Z @ Z.T + tauR * N * np.identity(F*M)) @ Z @ y\n",
    "        w[:] = np.linalg.solve(Z @ Z.T + tauR * N * np.identity(F*M), Z @ y)\n",
    "\n",
    "        loss.append(L(C, w, Z, XCb))\n",
    "        loss_split.append(Lsplit(C, w, Z, XCb))\n",
    "        \n",
    "    return C, w, Z, XCb, loss, loss_split\n",
    "\n",
    "test_optim(direct_split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
