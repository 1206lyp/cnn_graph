{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: learning graph filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random kNN graph: $W \\in \\mathbb{R}^{M \\times M} \\rightarrow L \\rightarrow U, \\Lambda$.\n",
    "2. Random graph signals: $X = \\{x_i\\}_{i=1}^N \\in \\mathbb{R}^{M \\times N}$.\n",
    "3. Linear mapping: $f(x_i, c) = U \\operatorname{diag}(c) U^T x_i$.\n",
    "4. Noisy target signals: $Y = \\{y_i\\}_{i=1}^N \\in \\mathbb{R}^{M \\times N}, y_i = f(x_i, c_{gt}) + \\mathcal{N}_M(0,\\epsilon)$.\n",
    "    1. With randomly generated coefficients $c_{gt} \\sim \\mathcal{N}_M(0,1)$.\n",
    "5. Convex and smooth loss function: $L = \\frac{1}{N} \\sum_{i=1}^N \\|f(x_i, c) - y_i\\|_2^2 = \\frac{1}{N} \\|U \\operatorname{diag}(c) U^TX - Y\\|_F^2$.\n",
    "    1. Gradient: $\\nabla_{c} L = \\frac{2}{N} \\left(U^T X \\circ ( c \\circ U^T X - U^T Y ) \\right) 1_N$.\n",
    "6. Optimization: $c^* = \\operatorname{arg min}_c L(c)$.\n",
    "7. Verification.\n",
    "    1. $c^*$ should converge to $c_{gt}$.\n",
    "    2. The loss $L(c^*)$ should converge to $L(c_{gt})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random, time\n",
    "import numpy as np\n",
    "import scipy.sparse, scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tol = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 100  # nodes\n",
    "degree = 5  # edges per vertex\n",
    "\n",
    "# Random connections and weights.\n",
    "I = np.arange(0, M).repeat(degree)\n",
    "J = np.random.randint(0, M, M*degree)\n",
    "V = np.random.uniform(0, 1, M*degree)\n",
    "W = scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n",
    "\n",
    "# No self-connections.\n",
    "W.setdiag(0)\n",
    "\n",
    "# Non-directed graph.\n",
    "bigger = W.T > W\n",
    "W = W - W.multiply(bigger) + W.T.multiply(bigger)\n",
    "del bigger\n",
    "assert np.abs(W - W.T).mean() < tol\n",
    "\n",
    "# CSR sparse matrix format for efficient multiplications.\n",
    "W = W.tocsr()\n",
    "W.eliminate_zeros()\n",
    "\n",
    "print(\"{} > {} edges\".format(W.nnz, M*degree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $L^\\text{unnormalized} = D - W$\n",
    "* $L^\\text{normalized} = I - D^{-1/2} W D^{-1/2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_laplacian = False\n",
    "\n",
    "# Degree matrix.\n",
    "d = W.sum(axis=0)\n",
    "\n",
    "# Laplacian matrix.\n",
    "if not normalized_laplacian:\n",
    "    D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "    L = D - W\n",
    "else:\n",
    "    d = 1 / np.sqrt(d)\n",
    "    D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "    I = scipy.sparse.identity(M, dtype=D.dtype)\n",
    "    L = I - D * W * D\n",
    "    del I\n",
    "del d, D\n",
    "\n",
    "# Symmetric matrix.\n",
    "assert np.abs(L - L.T).mean() < tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L = U^T \\Lambda U$ where $\\Lambda$ is a diagonal matrix of eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(lamb_s, U_s) = scipy.sparse.linalg.eigsh(L, k=M-1, which='SM')\n",
    "(lamb_sh, U_sh) = scipy.sparse.linalg.eigsh(L, k=M-1, which='SM')\n",
    "(lamb_h, U_h) = np.linalg.eigh(L.toarray())\n",
    "(lamb, U) = np.linalg.eig(L.toarray())  # Different result. Broken ?\n",
    "np.testing.assert_allclose(lamb_h[:-1], lamb_s, atol=tol)\n",
    "np.testing.assert_allclose(lamb_h[:-1], lamb_sh, atol=tol)\n",
    "#np.testing.assert_allclose(lamb_h, lamb, atol=tol)\n",
    "np.testing.assert_allclose(np.abs(U_h[:,:-1]), np.abs(U_s), atol=tol)\n",
    "np.testing.assert_allclose(np.abs(U_h[:,:-1]), np.abs(U_sh), atol=tol)\n",
    "#np.testing.assert_allclose(np.abs(U_h), np.abs(U), atol=tol)\n",
    "\n",
    "(lamb, U) = (lamb_h, U_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear mapping: graph filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear mapping $f(x_i, c) = U C U^T x_i$, $C$ is the diagonal matrix $C = \\operatorname{diag}(c)$, i.e. $c = C 1_M$.\n",
    "\n",
    "* Parametrized low-pass filter coefficients $(c_{gt})_i = \\operatorname{e}^{-t \\lambda_i}$\n",
    "* Random filter coefficients $c_{gt} \\sim \\mathcal{N}_M(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parametrized = False\n",
    "\n",
    "if parametrized:\n",
    "    t = 2\n",
    "    c_gt = np.exp(-t * lamb)\n",
    "    assert np.all(c[:-1] - c[1:] > 0)\n",
    "else:\n",
    "    c_gt = np.random.normal(0, 1, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random input signals $X \\sim \\mathcal{N}_{M \\times N}(0,1)$\n",
    "  * Low-pass signals ?\n",
    "* Noisy target signals $y_i = f(x_i, c_{gt}) + \\mathcal{N}_M(0,\\epsilon)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 200  # signals\n",
    "eps = 0.1  # noise\n",
    "\n",
    "X = np.random.normal(0, 1, (M,N))\n",
    "Y = U @ np.diag(c_gt) @ U.T @ X + (np.random.normal(0, eps, (M,N)) if eps > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loss function $L = \\frac{1}{N} \\sum_{i=1}^N \\|f(x_i, c) - y_i\\|_2^2 = \\frac{1}{N} \\|UCU^TX - Y\\|_F^2$.\n",
    "    * Spectral domain: $L = \\frac{1}{N} \\| C U^T X - U^T Y \\|_F^2$.\n",
    "    * Independant coefficients: $L = \\frac{1}{N} \\sum_{i=1}^M \\| c_i (U^T X)_{i,\\cdot} - (U^T Y)_{i,\\cdot} \\|_2^2$.\n",
    "    * Convex and smooth w.r.t. $c$.\n",
    "* Gradient:\n",
    "    * Independant coefficients: $\\nabla_{c_i} L = \\frac{2}{N} ( c_i (U^T X)_{i,\\cdot} - (U^T Y)_{i,\\cdot} ) (X^T U)_{\\cdot,i}$.\n",
    "    * $\\nabla_{c} L = \\frac{2}{N} \\left(U^T X \\circ ( c \\circ U^T X - U^T Y ) \\right) 1_N$.\n",
    "* Optimization $c^* = \\operatorname{arg min}_{c} L(c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def L(c):\n",
    "    M, N = X.shape\n",
    "    return np.linalg.norm(U @ np.diag(c) @ U.T @ X - Y, 'fro')**2 / N\n",
    "\n",
    "print(\"L(c_gt) = {}\".format(L(c_gt)))\n",
    "np.testing.assert_allclose(L(c_gt), M * eps**2, 1e-2)\n",
    "\n",
    "def dL(X, Y, c, variant=None):\n",
    "    M, N = X.shape\n",
    "    A = U.T @ X\n",
    "    B = U.T @ Y\n",
    "    # Speed: v3 >> v1 > v2.\n",
    "    if variant is 1:\n",
    "        return 2 / N * np.diag(A @ (A.T @ np.diag(c) - B.T))\n",
    "    elif variant is 2:\n",
    "        dc = np.empty(M)\n",
    "        for i in range(M):\n",
    "            dc[i] = 2 / N * (c[i] * A[i,:] - B[i,:]) @ A.T[:,i]\n",
    "        return dc\n",
    "    else:\n",
    "        # Speed: .sum(axis=1) is faster than *np.ones(N).y\n",
    "        return 2 / N * (A * (c[:,np.newaxis] * A - B)).sum(axis=1)\n",
    "\n",
    "# Gradient should be null at the global minimum. With noise, c_gt is not necessary the optimum.\n",
    "if eps <= 0:\n",
    "    np.testing.assert_allclose(dL(X, Y, c_gt), 0, atol=tol)\n",
    "np.testing.assert_allclose(dL(X, Y, c_gt), dL(X, Y, c_gt, 1))\n",
    "np.testing.assert_allclose(dL(X, Y, c_gt), dL(X, Y, c_gt, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: optimality condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Only possible because $L$ is convex and smooth.\n",
    "* Optimality condition $\\nabla_c L = 0$ gives $(U^T X \\circ U^T X) 1_N \\circ c = (U^T X \\circ U^T Y) 1_N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tstart = time.process_time()\n",
    "A = U.T @ X\n",
    "B = U.T @ Y\n",
    "c_opt = (A * B).sum(axis=1) / (A * A).sum(axis=1)\n",
    "print('Execution time: {:1.0f}ms'.format((time.process_time() - tstart) * 1000))\n",
    "\n",
    "print(\"L(c_opt) = {}\".format(L(c_opt)))\n",
    "np.testing.assert_allclose(dL(X, Y, c_opt), 0, atol=tol)\n",
    "if eps <= 0:\n",
    "    np.testing.assert_allclose(c_opt, c_gt)\n",
    "    np.testing.assert_allclose(L(c_opt), L(c_gt), atol=tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization: stochastic (mini-batch) gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Works also for $L$ which are non-smooth (with sub-gradient) or non-convex.\n",
    "* Idea: descend the gradient of the loss function.\n",
    "* Efficiency: compute the gradient $\\nabla_c L$ with a sub-set (mini-batch) of the training data.\n",
    "    * Extreme case: one sample at a time. Very inefficient.\n",
    "* Update rule (gradient descent) $c^{n+1} = c^n - \\lambda_n \\nabla_c L$.\n",
    "* Note: objective (loss on training set) and error (on validation set) are usually evaluated after each epoch. The algorithm is thus stopped after a maximum number of epochs rather than iterations.\n",
    "* Hyper-parameters.\n",
    "    * Learning rate (step size) $\\lambda_n$. Bigger the batch size, smaller the learning rate.\n",
    "        * Tradeoff.\n",
    "            * Small: progress is steady but slow.\n",
    "            * Big: risks of oscillations or divergence.\n",
    "        * There are tricks, e.g. vanishing step (like simulated annealing).\n",
    "    * Size of the mini-batch.\n",
    "        * We want the one who minimizes the *training time*.\n",
    "        * Trade-off: should be limited by the available memory, somewhere around 100.\n",
    "            * Larger is more stable, but computationnaly more expensive.\n",
    "            * Smaller demands more accesses to memory, which is slow.\n",
    "            * Larger exploits the parallelism of modern hardware architectures (SIMD on CPU, GPU).\n",
    "        * Extreme cases:\n",
    "            * $1$: stochastic gradient descent.\n",
    "            * $N$: gradient descent.\n",
    "    * Stopping criterion.\n",
    "        * Convergence of the loss function $L$.\n",
    "        * Convergence of the parameters $c$.\n",
    "        * Maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(learning_rate=.1, batch_size=100, conv=1e-3, maxit=100, window=10):\n",
    "    \"\"\"Stochastic (mini-batch) gradient descent.\"\"\"\n",
    "    indices = []\n",
    "    c = np.random.uniform(0, 1, M)\n",
    "    loss = [L(c)]\n",
    "    \n",
    "    def stop(loss):\n",
    "        \"\"\"Stop after convergence of the loss.\"\"\"\n",
    "        if len(loss) > maxit:\n",
    "            return True\n",
    "        elif len(loss) >= 2 * window:\n",
    "            avg1 = np.mean(loss[-window:])\n",
    "            avg2 = np.mean(loss[-2*window:-window])\n",
    "            return True if avg2 - avg1 < conv else False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    while not stop(loss):\n",
    "        \n",
    "        # Be sure to have used all the samples before using one a second time.\n",
    "        if len(indices) < batch_size:\n",
    "            new_indices = np.arange(N)\n",
    "            np.random.shuffle(new_indices)\n",
    "            indices.extend(new_indices)\n",
    "        idx = indices[:batch_size]\n",
    "        del indices[:batch_size]\n",
    "        \n",
    "        c -= learning_rate * dL(X[:,idx], Y[:,idx], c)\n",
    "        loss.append(L(c))\n",
    "        \n",
    "    return c, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxit = 40\n",
    "\n",
    "def plot(learning_rate, batch_size):\n",
    "    tstart = time.process_time()\n",
    "    (c, loss) = sgd(learning_rate, batch_size, 1e-3, maxit)\n",
    "    t = (time.process_time() - tstart) * 1000\n",
    "    plt.plot(loss, label='rate={}, size={}, L(c)={:1.2e}, time={:1.0f}ms'.format(learning_rate, batch_size, L(c), t))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plot(0.2, 1)\n",
    "plot(0.2, 5)\n",
    "plot(0.2, 50)\n",
    "plot(0.2, 100)\n",
    "plot(0.6, 100)\n",
    "plt.plot(L(c_opt) * np.ones(maxit+1), label='optimality condition')\n",
    "plt.plot(L(c_gt) * np.ones(maxit+1), label='ground truth')\n",
    "plt.title('Convergence, M={}, N={}, eps={}'.format(M, N, eps))\n",
    "plt.xlabel('iteration n')\n",
    "plt.ylabel('loss L(c^n)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* Noise: why don't we find the same loss as the ground truth, but the same as linear programming ?\n",
    "    * The gradient was incorrectly set to $\\nabla_c L = \\frac{2}{N} U^T X (X^T U c - Y^T U 1_M)$.\n",
    "* More samples, e.g. $N=2000$: why don't we find the same loss as the linear programm ?\n",
    "    * Learning rate too high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
