{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: learning graph filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random kNN graph: $W \\in \\mathbb{R}^{M \\times M} \\rightarrow L \\rightarrow U, \\Lambda$.\n",
    "2. Random graph signals: $X = \\{x_i\\}_{i=1}^N \\in \\mathbb{R}^{M \\times N}$.\n",
    "3. Linear mapping: $f(x_i, c) = U \\operatorname{diag}(c) U^T x_i$.\n",
    "4. Noisy target signals: $Y = \\{y_i\\}_{i=1}^N \\in \\mathbb{R}^{M \\times N}, y_i = f(x_i, c_{gt}) + \\mathcal{N}_M(0,\\epsilon)$.\n",
    "    1. With randomly generated coefficients $c_{gt} \\sim \\mathcal{N}_M(0,1)$.\n",
    "5. Convex and smooth loss function: $L = \\frac{1}{N} \\sum_{i=1}^N \\|f(x_i, c) - y_i\\|_2^2 = \\frac{1}{N} \\|U \\operatorname{diag}(c) U^TX - Y\\|_F^2$.\n",
    "    1. Gradient: $\\nabla_{c} L = \\frac{2}{N} \\left(U^T X \\circ ( c \\circ U^T X - U^T Y ) \\right) 1_N$.\n",
    "6. Optimization: $c^* = \\operatorname{arg min}_c L(c)$.\n",
    "7. Verification.\n",
    "    1. $c^*$ should converge to $c_{gt}$.\n",
    "    2. The loss $L(c^*)$ should converge to $L(c_{gt})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random, time\n",
    "import numpy as np\n",
    "import scipy.sparse, scipy.sparse.linalg, scipy.spatial.distance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tol = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setting\n",
    "\n",
    "### Graph\n",
    "\n",
    "* A completely random graph is not smooth at all and will thus have a large spectral gap, i.e. $\\lambda_1 >> \\lambda_0$.\n",
    "* A grid, on the contrary, is very regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 100  # nodes\n",
    "k = 4  # edges per vertex\n",
    "\n",
    "def graph_random():\n",
    "    \"\"\"Random connections and weights.\"\"\"\n",
    "    I = np.arange(0, M).repeat(k)\n",
    "    J = np.random.randint(0, M, M*k)\n",
    "    V = np.random.uniform(0, 1, M*k)\n",
    "    return scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n",
    "\n",
    "def graph_grid():\n",
    "    \"\"\"Construct a kNN graph aranged on a 2D grid.\"\"\"\n",
    "    \n",
    "    # Construct a grid.\n",
    "    m = np.int(np.sqrt(M))\n",
    "    x = np.linspace(0,1,m)\n",
    "    y = np.linspace(0,1,m)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = np.empty((M,2))\n",
    "    z[:,0] = xx.reshape(M)\n",
    "    z[:,1] = yy.reshape(M)\n",
    "\n",
    "    # Compute pairwise distances.\n",
    "    d = scipy.spatial.distance.pdist(z, 'euclidean')\n",
    "    d = scipy.spatial.distance.squareform(d)\n",
    "\n",
    "    # k-NN graph.\n",
    "    idx = np.argsort(d)[:,1:k+1]\n",
    "    d.sort()\n",
    "    d = d[:,1:k+1]\n",
    "\n",
    "    # Weights.\n",
    "    sigma2 = np.mean(d[:,-1])**2\n",
    "    d = np.exp(- d**2 / sigma2)\n",
    "\n",
    "    # Weight matrix.\n",
    "    I = np.arange(0, M).repeat(k)\n",
    "    J = idx.reshape(M*k)\n",
    "    V = d.reshape(M*k)\n",
    "    return scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n",
    "\n",
    "W = graph_grid()\n",
    "\n",
    "# No self-connections.\n",
    "W.setdiag(0)\n",
    "\n",
    "# Non-directed graph.\n",
    "bigger = W.T > W\n",
    "W = W - W.multiply(bigger) + W.T.multiply(bigger)\n",
    "del bigger\n",
    "assert np.abs(W - W.T).mean() < tol\n",
    "\n",
    "# CSR sparse matrix format for efficient multiplications.\n",
    "W = W.tocsr()\n",
    "W.eliminate_zeros()\n",
    "\n",
    "print(\"{} > {} edges\".format(W.nnz, M*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $L^\\text{unnormalized} = D - W$\n",
    "* $L^\\text{normalized} = I - D^{-1/2} W D^{-1/2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_laplacian = True\n",
    "\n",
    "def laplacian(W, normalized=True):\n",
    "    \"\"\"Return the Laplacian of the weigth matrix.\"\"\"\n",
    "    \n",
    "    # Degree matrix.\n",
    "    d = W.sum(axis=0)\n",
    "\n",
    "    # Laplacian matrix.\n",
    "    if not normalized:\n",
    "        D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "        return D - W\n",
    "    else:\n",
    "        d = 1 / np.sqrt(d)\n",
    "        D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "        I = scipy.sparse.identity(M, dtype=D.dtype)\n",
    "        return I - D * W * D\n",
    "\n",
    "\n",
    "t_start = time.process_time()\n",
    "LL = laplacian(W, normalized_laplacian)\n",
    "print('Execution time: {:1.0f}ms'.format((time.process_time() - t_start) * 1000))\n",
    "assert np.abs(LL - LL.T).mean() < tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L = U^T \\Lambda U$ where $\\Lambda$ is a diagonal matrix of eigenvalues.\n",
    "Compare the results of four algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fourier(L):\n",
    "\n",
    "    def sort(lamb, U):\n",
    "        idx = lamb.argsort()\n",
    "        return lamb[idx], U[:,idx]\n",
    "\n",
    "    t_start = time.process_time()\n",
    "    lamb, U = np.linalg.eig(LL.toarray())\n",
    "    lamb, U = sort(lamb, U)\n",
    "    print('Execution time: {:1.0f}ms'.format((time.process_time() - t_start) * 1000))\n",
    "\n",
    "    if M <= 100:  # Because of the computational complexity.\n",
    "        \n",
    "        lamb_, U_ = np.linalg.eigh(LL.toarray())\n",
    "        np.testing.assert_allclose(lamb_, lamb, atol=tol)\n",
    "        np.testing.assert_allclose(np.abs(U_), np.abs(U), atol=tol)\n",
    "\n",
    "        lamb_, U_ = scipy.sparse.linalg.eigs(LL, k=M-2, which='SM')\n",
    "        lamb_, U_ = sort(lamb_, U_)\n",
    "        np.testing.assert_allclose(lamb[:-2], lamb_, atol=tol)\n",
    "        np.testing.assert_allclose(np.abs(U[:,:-2]), np.abs(U_), atol=tol)\n",
    "\n",
    "        lamb_, U_ = scipy.sparse.linalg.eigsh(LL, k=M-1, which='SM')\n",
    "        np.testing.assert_allclose(lamb[:-1], lamb_, atol=tol)\n",
    "        np.testing.assert_allclose(np.abs(U[:,:-1]), np.abs(U_), atol=tol)\n",
    "    \n",
    "    return lamb, U\n",
    "\n",
    "lamb, U = fourier(LL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper-bound approximation of the spectrum.\n",
    "\n",
    "* Computed by the Implicitly Restarted Lanczos Method (IRLM), which is a reduction of a variant of the Arnoldi iteration. It is faster than the Power method.\n",
    "* Normalized graph Laplacian has a bounded spectrum $0 \\leq \\lambda \\leq 2$.\n",
    "* `eigs` is faster than `eigsh`. There are also non-sparse routines in `scipy.linalg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_start = time.process_time()\n",
    "lmax = scipy.sparse.linalg.eigsh(LL, k=1, which='LM', return_eigenvectors=False)[0]\n",
    "print('Execution time: {:1.0f}ms'.format((time.process_time() - t_start) * 1000))\n",
    "if normalized_laplacian:\n",
    "    assert lmax <= 2\n",
    "print('Spectrum: [{:1.2e}, {:1.2e}]'.format(lamb[0], lmax))\n",
    "np.testing.assert_allclose(lamb[0], 0, atol=tol)\n",
    "np.testing.assert_allclose(lamb[-1], lmax, atol=tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth graph filter\n",
    "\n",
    "Linear mapping $f(x_i, c) = U C U^T x_i$, $C$ is the diagonal matrix $C = \\operatorname{diag}(c)$, i.e. $c = C 1_M$.\n",
    "\n",
    "* Parametrized low-pass filter coefficients $(c_{gt})_i = \\operatorname{e}^{-t \\lambda_i}$\n",
    "* Random filter coefficients $c_{gt} \\sim \\mathcal{N}_M(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parametrized = True\n",
    "\n",
    "if parametrized:\n",
    "    def g(x, t=.5):\n",
    "        return np.sin(4 * (x-2)**2)\n",
    "        return np.sin(2 * (x-2)**2)\n",
    "        return np.exp(-t * x)\n",
    "    c_g = g(lamb)\n",
    "    #assert np.all(c_gt[:-1] - c_gt[1:] > 0)\n",
    "else:\n",
    "    c_g = np.random.normal(0, 1, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signals\n",
    "\n",
    "* Random input signals $X \\sim \\mathcal{N}_{M \\times N}(0,1)$\n",
    "  * Low-pass signals ?\n",
    "* Noisy target signals $y_i = f(x_i, c_{gt}) + \\mathcal{N}_M(0,\\epsilon)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 200  # signals\n",
    "eps = 0.1  # noise\n",
    "\n",
    "X = np.random.normal(0, 1, (M,N))\n",
    "Y = U @ np.diag(c_g) @ U.T @ X + (np.random.normal(0, eps, (M,N)) if eps > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametrized filter learning\n",
    "\n",
    "### Loss function\n",
    "\n",
    "* Loss function $L = \\frac{1}{N} \\sum_{i=1}^N \\|f(x_i, c) - y_i\\|_2^2 = \\frac{1}{N} \\|UCU^TX - Y\\|_F^2$.\n",
    "    * Spectral domain: $L = \\frac{1}{N} \\| C U^T X - U^T Y \\|_F^2$.\n",
    "    * Independant coefficients: $L = \\frac{1}{N} \\sum_{i=1}^M \\| c_i (U^T X)_{i,\\cdot} - (U^T Y)_{i,\\cdot} \\|_2^2$.\n",
    "    * Convex and smooth w.r.t. $c$.\n",
    "* Gradient:\n",
    "    * Independant coefficients: $\\nabla_{c_i} L = \\frac{2}{N} ( c_i (U^T X)_{i,\\cdot} - (U^T Y)_{i,\\cdot} ) (X^T U)_{\\cdot,i}$.\n",
    "    * $\\nabla_{c} L = \\frac{2}{N} \\left(U^T X \\circ ( c \\circ U^T X - U^T Y ) \\right) 1_N$.\n",
    "* Optimization $c^* = \\operatorname{arg min}_{c} L(c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_full(X, c):\n",
    "    \"\"\"Filter X with a full spectral domain filter.\"\"\"\n",
    "    return U @ np.diag(c) @ U.T @ X\n",
    "np.testing.assert_allclose(filter_full(X, np.ones(M)), X, atol=tol)\n",
    "\n",
    "def L(c):\n",
    "    M, N = X.shape\n",
    "    return np.linalg.norm(filter_full(X, c) - Y, ord='fro')**2 / N\n",
    "np.testing.assert_allclose(L(c_g), M * eps**2, 2e-2)\n",
    "\n",
    "def dL(X, Y, c, variant=None):\n",
    "    M, N = X.shape\n",
    "    A = U.T @ X\n",
    "    B = U.T @ Y\n",
    "    # Speed: v3 >> v1 > v2.\n",
    "    if variant is 1:\n",
    "        return 2 / N * np.diag(A @ (A.T @ np.diag(c) - B.T))\n",
    "    elif variant is 2:\n",
    "        dc = np.empty(M)\n",
    "        for i in range(M):\n",
    "            dc[i] = 2 / N * (c[i] * A[i,:] - B[i,:]) @ A.T[:,i]\n",
    "        return dc\n",
    "    else:\n",
    "        # Speed: .sum(axis=1) is faster than *np.ones(N).y\n",
    "        return 2 / N * (A * (c[:,np.newaxis] * A - B)).sum(axis=1)\n",
    "# Gradient should be null at the global minimum. With noise, c_g is not necessary the optimum.\n",
    "if eps <= 0:\n",
    "    np.testing.assert_allclose(dL(X, Y, c_g), 0, atol=tol)\n",
    "np.testing.assert_allclose(dL(X, Y, c_g), dL(X, Y, c_g, 1))\n",
    "np.testing.assert_allclose(dL(X, Y, c_g), dL(X, Y, c_g, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: optimality condition\n",
    "\n",
    "* Only possible because $L$ is convex and smooth.\n",
    "* Optimality condition $\\nabla_c L = 0$ gives $(U^T X \\circ U^T X) 1_N \\circ c = (U^T X \\circ U^T Y) 1_N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_start = time.process_time()\n",
    "A = U.T @ X\n",
    "B = U.T @ Y\n",
    "c_o = (A * B).sum(axis=1) / (A * A).sum(axis=1)\n",
    "print('Execution time: {:1.0f}ms'.format((time.process_time() - t_start) * 1000))\n",
    "\n",
    "assert L(c_o) < L(c_g) + tol\n",
    "assert np.linalg.norm(dL(X, Y, c_o)) < np.linalg.norm(dL(X, Y, c_g))\n",
    "np.testing.assert_allclose(dL(X, Y, c_o), 0, atol=tol)\n",
    "if eps <= 0:\n",
    "    np.testing.assert_allclose(c_o, c_g, atol=tol)\n",
    "    np.testing.assert_allclose(L(c_o), L(c_g), atol=tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: stochastic (mini-batch) gradient descent\n",
    "\n",
    "* Works also for $L$ which are non-smooth (with sub-gradient) or non-convex.\n",
    "* Idea: descend the gradient of the loss function.\n",
    "* Efficiency: compute the gradient $\\nabla_c L$ with a sub-set (mini-batch) of the training data.\n",
    "    * Extreme case: one sample at a time. Very inefficient.\n",
    "* Update rule (gradient descent) $c^{n+1} = c^n - \\lambda_n \\nabla_c L$.\n",
    "* Note: objective (loss on training set) and error (on validation set) are usually evaluated after each epoch. The algorithm is thus stopped after a maximum number of epochs rather than iterations.\n",
    "* Hyper-parameters.\n",
    "    * Learning rate (step size) $\\lambda_n$. Bigger the batch size, smaller the learning rate.\n",
    "        * Tradeoff.\n",
    "            * Small: progress is steady but slow.\n",
    "            * Big: risks of oscillations or divergence.\n",
    "        * There are tricks, e.g. vanishing step (like simulated annealing).\n",
    "    * Size of the mini-batch.\n",
    "        * We want the one who minimizes the *training time*.\n",
    "        * Trade-off: should be limited by the available memory, somewhere around 100.\n",
    "            * Larger is more stable, but computationnaly more expensive.\n",
    "            * Smaller demands more accesses to memory, which is slow.\n",
    "            * Larger exploits the parallelism of modern hardware architectures (SIMD on CPU, GPU).\n",
    "        * Extreme cases:\n",
    "            * $1$: stochastic gradient descent.\n",
    "            * $N$: gradient descent.\n",
    "    * Stopping criterion.\n",
    "        * Convergence of the loss function $L$.\n",
    "        * Convergence of the parameters $c$.\n",
    "        * Maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(c0, L, dL, learning_rate=.1, batch_size=100, conv=1e-3, maxit=100, window=10):\n",
    "    \"\"\"Stochastic (mini-batch) gradient descent.\"\"\"\n",
    "    indices = []\n",
    "    c = c0\n",
    "    loss = [L(c)]\n",
    "    \n",
    "    def stop(loss):\n",
    "        \"\"\"Stop after convergence of the loss.\"\"\"\n",
    "        if len(loss) > maxit:\n",
    "            return True\n",
    "        #elif np.linalg.norm(dL(X, Y, c)) < conv:\n",
    "            #return True\n",
    "        elif len(loss) >= 2 * window:\n",
    "            avg1 = np.mean(loss[-window:])\n",
    "            avg2 = np.mean(loss[-2*window:-window])\n",
    "            return True if avg2 - avg1 < conv else False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    while not stop(loss):\n",
    "        \n",
    "        # Be sure to have used all the samples before using one a second time.\n",
    "        if len(indices) < batch_size:\n",
    "            new_indices = np.arange(N)\n",
    "            np.random.shuffle(new_indices)\n",
    "            indices.extend(new_indices)\n",
    "        idx = indices[:batch_size]\n",
    "        del indices[:batch_size]\n",
    "        \n",
    "        c -= learning_rate * dL(X[:,idx], Y[:,idx], c)\n",
    "        loss.append(L(c))\n",
    "        \n",
    "    return c, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_plot_convergence(c0, L, dL, params, conv, maxit):\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    c_sgd = []\n",
    "    mlen = 0\n",
    "    for param in params:\n",
    "        t_start = time.process_time()\n",
    "        c, loss = sgd(np.array(c0), L, dL, param[0], param[1], conv, maxit)\n",
    "        t = (time.process_time() - t_start) * 1000\n",
    "        label = 'rate={}, size={}, L(c)={:1.2e}, |dL(c)|={:1.2e}, time={:1.0f}ms'.format(\n",
    "            param[0], param[1], L(c), np.linalg.norm(dL(X, Y, c)), t)\n",
    "        ax.plot(loss, label=label)\n",
    "        c_sgd.append(c)\n",
    "        mlen = max(mlen, len(loss))\n",
    "        \n",
    "    ax.set_title('Convergence, M={}, N={}, eps={}'.format(M, N, eps))\n",
    "    ax.set_xlabel('iteration n')\n",
    "    ax.set_ylabel('loss L(c^n)')\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlim(0, mlen-1)\n",
    "    plt.show()\n",
    "    \n",
    "    return c_sgd\n",
    "\n",
    "params = []\n",
    "params.append([0.2, 1])\n",
    "params.append([0.2, 5])\n",
    "params.append([0.2, 50])\n",
    "params.append([0.2, 100])\n",
    "params.append([0.6, 100])\n",
    "c0 = np.random.uniform(0, 1, M)\n",
    "c_s = sgd_plot_convergence(c0, L, dL, params, conv=1e-3, maxit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: learned filters\n",
    "\n",
    "Observations:\n",
    "* Noise: why don't we find the same loss as the ground truth, but the same as linear programming ?\n",
    "    * The gradient was incorrectly set to $\\nabla_c L = \\frac{2}{N} U^T X (X^T U c - Y^T U 1_M)$.\n",
    "* More samples, e.g. $N=2000$: why don't we find the same loss as the linear program ?\n",
    "    * Learning rate too high.\n",
    "* The spectral gap $\\lambda_1$ is large for a random graph.\n",
    "* Without noise, the recovered filter is exact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_filters(coeffs):\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    for coeff in coeffs:\n",
    "        c = eval(coeff)\n",
    "        label = '{}: L={:1.2e}, |dL|={:1.2e}'.format(coeff, L(c), np.linalg.norm(dL(X,Y,c)))\n",
    "        ax.plot(lamb, c, '.-', label=label)\n",
    "    ax.set_xlim(lamb[0], lamb[-1])\n",
    "    ax.set_title('Filter coefficients, M={}, N={}, eps={}'.format(M, N, eps))\n",
    "    ax.set_xlabel('frequency')\n",
    "    ax.set_ylabel('amplitude')\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "plot_filters(['c_s[4]', 'c_s[0]', 'c_o', 'c_g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrized filter learning: truncated Chebyshev expansion\n",
    "\n",
    "* Use a $K$th order polynomial approximation of the filter.\n",
    "* Less free parameters: $K << M$.\n",
    "* Good approximation for smooth, i.e. localized, filters.\n",
    "\n",
    "### Basis of Chebyshev polynomials\n",
    "\n",
    "* Compute the Chebyshev basis $T$ of order $K$.\n",
    "* This basis will allow us to construct and observe the filter from the inferred polynomial coefficients.\n",
    "* The figure shows that we indeed generate the Chebyshev polynomials of the first kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "\n",
    "def cheby_basis(K, x):\n",
    "    \"\"\"Return the Chebyshev basis of order K (composed of the\n",
    "    first K polynomials) evaluated at x. Polynomials are generated\n",
    "    by their recursive formulation.\"\"\"\n",
    "    T = np.empty((x.size, K))\n",
    "    T[:,0] = np.ones(x.size)\n",
    "    if K >= 2:\n",
    "        T[:,1] = x\n",
    "    for k in range(2, K):\n",
    "        T[:,k] = 2 * x * T[:,k-1] - T[:,k-2]\n",
    "    return T\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "x = np.linspace(-1,1,100)\n",
    "T = cheby_basis(K, x)\n",
    "for k in range(K):\n",
    "    ax.plot(x, T[:,k], label='T_{}'.format(k))\n",
    "ax.set_title('Chebyshev polynomials of the first kind')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('T_n(x)')\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1.1)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth Chebyshev expansion's coefficients\n",
    "\n",
    "Given the filter $g$ with a vector $c_{gt} \\in \\mathbb{R}^M$ of evaluations, find the Chebyshev coefficients $c_{cgt} \\in \\mathbb{R}^K$. Truncated Chebyshev series closely approximate the minimax polynomial, i.e. $c_{cgt} \\approx \\operatorname{arg min}_c \\| c_{gt} - \\sum_k c_k T_k \\|_\\infty$ where $T_k$ is the Chebyshev polynomial of order $k$. Given that the polynomials form an orthogonal basis for $L^2([-1,1],\\frac{dy}{\\sqrt{1-y^r}})$, the coefficients can be retrieved by two methods.\n",
    "\n",
    "1. Analytical projection.\n",
    "    * $c_k = \\frac{2}{\\pi} \\int_0^\\pi \\cos(k\\theta) g( \\frac{\\lambda_{max}}{2} (\\cos(\\theta) + 1)) d\\theta$\n",
    "    * Need the analytic function.\n",
    "2. Numerical projection (discrete orthogonality condition).\n",
    "    * $c_k = \\frac{2}{K} \\sum_j g(x_j) T_k(x_j)$ where the $x_j$ are the $K$ Chebyshev nodes, because the approximation error is null only at these points.\n",
    "    * Need function evaluations at the Chebyshev nodes, but those only. Much less points than least mean square.\n",
    "\n",
    "In our setting, the generative filter is the function to learn. We have however access to some evaluations of the filter (at the eigenvalues of the Laplacian) via convex optimization of the loss function $L$ (described above). From those, given the Chebyshev basis, we can retrieve the coefficients that minimize the reconstruction error of this filter.\n",
    "\n",
    "Results:\n",
    "\n",
    "* Playing with the order $K$ shows that the approximation converges to the filter $g$.\n",
    "* The approximation constructed by minimizing the filter l2 reconstruction error is now longer a Chebyshev polynomial (there are error on the Chebyshev nodes) but it provides a smaller loss $L$ (our final measure of quality). It however requires the full Chebyshev basis, which requires the eigenvalues of the Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "def rescale(x, reverse=False):\n",
    "    \"\"\"Rescale the spectral domain to [-1,1].\"\"\"\n",
    "    if normalized_laplacian:\n",
    "        lmax = 2\n",
    "    if reverse:\n",
    "        return x / lmax * 2 - 1\n",
    "    else:\n",
    "        return (x + 1) / 2 * lmax\n",
    "np.testing.assert_allclose(lamb, rescale(rescale(lamb, True)), atol=tol)\n",
    "\n",
    "def cheby_nodes(K):\n",
    "    \"\"\"Return the K Chebyshev nodes in [-1,1].\"\"\"\n",
    "    return np.cos(np.pi * (np.arange(K) + 1/2) / K)\n",
    "    \n",
    "def cheby_coeff(K, f):\n",
    "    \"\"\"Compute the coefficients of the Chebyshev polynomial approximation.\"\"\"\n",
    "    # Coefficients from discrete orthogonality condition.\n",
    "    # It can be done faster via the discrete cosine transform.\n",
    "    c = np.empty(K)\n",
    "    x = cheby_nodes(K)\n",
    "    T = cheby_basis(K, x)\n",
    "    for k in range(K):\n",
    "        c[k] = 2 / K * np.sum(f(x) * T[:,k])\n",
    "    c[0] /= 2\n",
    "    return c\n",
    "\n",
    "# Domain is [-1, 1].\n",
    "x = np.linspace(-1,1,100)\n",
    "x = rescale(lamb, True)\n",
    "f = lambda x: g(rescale(x))\n",
    "np.testing.assert_allclose(f(x), c_g)\n",
    "\n",
    "c_cg = cheby_coeff(K, f)\n",
    "np.testing.assert_allclose(f(cheby_nodes(K)), cheby_basis(K, cheby_nodes(K)) @ c_cg)\n",
    "\n",
    "T = cheby_basis(K, x)\n",
    "c_co = np.linalg.lstsq(T, c_g)[0]\n",
    "\n",
    "plot_filters(['T @ c_co', 'T @ c_cg', 'c_g'])\n",
    "plt.plot(rescale(cheby_nodes(K)), f(cheby_nodes(K)), 'k.', markersize=15, label='Chebyshev nodes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial order\n",
    "\n",
    "Determine the polynomial order by filtering the data with Chebyshev approximations of order $1 \\leq k \\leq K$ and monitoring the reconstruction loss $L$.\n",
    "\n",
    "* The result shows that the approximation does indeed converge.\n",
    "* The approximation loss arrives at a plateau (the round-off error ?) given a high enough order.\n",
    "* As anticipated on the figure above, the coefficients provided by least square reconstruction have smaller loss than the *correct* ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def polynomial_order(K):\n",
    "    loss_cg = np.empty((K))\n",
    "    loss_co = np.empty((K))\n",
    "    for k in range(1, K+1):\n",
    "        T = cheby_basis(k, x)\n",
    "        c_cg = cheby_coeff(k, f)\n",
    "        loss_cg[k-1] = L(T @ c_cg)\n",
    "        c_co = np.linalg.lstsq(T, f(x))[0]\n",
    "        loss_co[k-1] = L(T @ c_co)\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.semilogy(range(1,K+1), loss_cg, label='L(T @ c_cg)')\n",
    "    ax.semilogy(range(1,K+1), loss_co, label='L(T @ c_co)')\n",
    "    ax.semilogy(L(c_o) * np.ones(K+1), label='L(c_o)')\n",
    "    ax.set_title('Loss due to Chebyshev approximation')\n",
    "    ax.set_xlabel('Polynomial order')\n",
    "    ax.set_ylabel('Loss L')\n",
    "    ax.set_xlim(1, K)\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "polynomial_order(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the polynomial order $K$ and compute the basis $T$ with their associate coefficients $c_{cgt}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 15\n",
    "t_start = time.process_time()\n",
    "c_cg = cheby_coeff(K, f)\n",
    "T = cheby_basis(K, x)\n",
    "print('Execution time: {:1.0f}ms'.format((time.process_time() - t_start) * 1000))\n",
    "\n",
    "# If the order is sufficient for a perfect (as good as c_gt) reconstruction (test only).\n",
    "pol_order_is_sufficient = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "* Independant coefficients: $L = \\frac{1}{N} \\sum_{i=1}^M \\| (Tc)_i (U^T X)_{i,\\cdot} - (U^T Y)_{i,\\cdot} \\|_2^2$.\n",
    "* $L = \\frac{1}{N} \\| Tc \\circ U^T X - U^T Y \\|_2^2$.\n",
    "* $\\nabla_{c} L = \\frac{2}{N} \\left(T^T \\left( U^T X \\circ ( Tc \\circ U^T X - U^T Y ) \\right) \\right) 1_N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_chebyshev(X, c):\n",
    "    \"\"\"Filter X with the Chebyshev coefficients of the full filter.\"\"\"\n",
    "    return filter_full(X, T @ c)\n",
    "c = np.zeros(K)\n",
    "c[0] = 1\n",
    "np.testing.assert_allclose(filter_chebyshev(X, c), X, atol=tol)\n",
    "\n",
    "def Lc(c):\n",
    "    M, N = X.shape\n",
    "    return np.linalg.norm(filter_chebyshev(X, c) - Y, ord='fro')**2 / N\n",
    "np.testing.assert_allclose(Lc(c_cg), L(T @ c_cg), atol=tol)\n",
    "if pol_order_is_sufficient:\n",
    "    np.testing.assert_allclose(Lc(c_cg), M * eps**2, rtol=1e-2, atol=tol)\n",
    "    np.testing.assert_allclose(Lc(c_cg), L(c_g), atol=tol)\n",
    "\n",
    "def dLc(X, Y, c):\n",
    "    M, N = X.shape\n",
    "    A = U.T @ X\n",
    "    B = U.T @ Y\n",
    "    return 2 / N * T.T @ (A * ((T @ c)[:,np.newaxis] * A - B)).sum(axis=1)\n",
    "# Gradient should be null at the global minimum. With noise, c_cg is not necessary the optimum.\n",
    "if eps <= 0 and pol_order_is_sufficient:\n",
    "    np.testing.assert_allclose(dLc(X, Y, c_cg), 0, atol=tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimality condition\n",
    "\n",
    "* Given the signals $X$, $Y$ and the Chebyshev basis $T$, find the Chebyshev coefficients $c_{copt}$.\n",
    "* Optimality condition $\\nabla_c L = 0$ gives $(U^T X \\circ U^T X) 1_N \\circ Tc = (U^T X \\circ U^T Y) 1_N$.\n",
    "* Why do we not always reach the minimum, i.e. $\\nabla_c L = 0$ ? E.g. for small polynomial orders, when we are not able to sufficiently approximate the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_start = time.process_time()\n",
    "c_co = np.linalg.lstsq(T, c_o)[0]\n",
    "print('Execution time: {:1.0f}ms'.format((time.process_time() - t_start) * 1000))\n",
    "\n",
    "assert Lc(c_co) < Lc(c_cg) + tol\n",
    "assert np.linalg.norm(dLc(X, Y, c_co)) < np.linalg.norm(dLc(X, Y, c_cg))\n",
    "#np.testing.assert_allclose(dLc(X, Y, c_co), 0, atol=tol)\n",
    "if eps <= 0 and pol_order_is_sufficient:\n",
    "    np.testing.assert_allclose(Lc(c_co), Lc(c_cg), atol=tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "* Why |dL(c)| does not converge to the null vector ? There should be no gradient at the optimum.\n",
    "* Convergence seems harder than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c0 = np.random.uniform(0, 1, K)\n",
    "c_cs = sgd_plot_convergence(c0, Lc, dLc, [[0.005, 100]], conv=1e-3, maxit=100)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: learned filters\n",
    "\n",
    "* The coefficients `c_co`, being optimal, alwas have a smallest loss than the ground truth `c_cg` (interpolant at the Chebyshev points).\n",
    "* The SGD solution `c_cs` does not converge exactly to `c_co`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_coefficients(coeffs):\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    for coeff in coeffs:\n",
    "        c = eval(coeff)\n",
    "        label = '{}: L={:1.2e}, |dL|={:1.2e}'.format(coeff, Lc(c), np.linalg.norm(dLc(X,Y,c)))\n",
    "        ax.plot(c, 'o', label=label)\n",
    "    ax.set_xlim(-1, K)\n",
    "    ax.set_title('Chebyshev expansion coefficients, M={}, N={}, K={}, eps={}'.format(M, N, K, eps))\n",
    "    ax.set_xlabel('number')\n",
    "    ax.set_ylabel('value')\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "plot_coefficients(['c_cs', 'c_co', 'c_cg'])\n",
    "plot_filters(['T @ c_cs', 'T @ c_co', 'T @ c_cg', 'c_o'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter learning: recursive Chebyshev expansion\n",
    "\n",
    "* Compute the coefficients via the recursive generation formula of the Chebyshev polynomials.\n",
    "* Goals:\n",
    "    * Avoid the $O(M^2)$ filtering operation, i.e. the multiplication with the Fourier basis to transform the signal in the spectral domain (convolution with the filter in the spatial domain).\n",
    "        * Avoid the expensive EVD of the large (but sparse) Laplacian matrix needed to retrieve the Fourier basis. \n",
    "    * Avoid the computation of the Chebyshev basis.\n",
    "\n",
    "Steps:\n",
    "\n",
    "* Estimate the upper bound $\\lambda_{max}$ of the Laplacian spectrum, i.e. an upper bound on the largest eigenvalue $\\lambda_{M-1}$.\n",
    "    * Normalized Laplacian spectrum is upper bounded by 2.\n",
    "* We will need the adjoint for back-propagation.\n",
    "\n",
    "Overall complexity from $O(M^2 \\max(M,N))$ to $O(KMN)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chebyshev recursive formula\n",
    "\n",
    "Step 1: Given the Chebyshev coefficients $c$, recursively refine $X$ into $\\hat{Y}$. Our goal is to avoid the expensive $O(M^2)$ multiplication by the Fourier basis $U$.\n",
    "\n",
    "* Compute the projection of the signal on the Chebyshev basis: $\\hat{Y}_k = T_k X$. Keeping this result allows fast application of multiple set of coefficients.\n",
    "    * The trick here is that each $\\hat{Y}_k$ is recursively generated from $\\hat{Y}_{k-1}$ and $\\hat{Y}_{k-2}$ with only a multiplication by the sparse Laplacian matrix $\\mathcal{L}$.\n",
    "    * This operation thus costs $O(K |E| N) = O(KMN)$ operations if the number of edges $|E|$ is proportional to the number of nodes $M$ (e.g. for kNN graphs) while transforming the signals into the Fourier domain costs $O(M^2N)$.\n",
    "    * This is done once for every signal. The filter coefficients $c$ can then be applied multiple times during learning.\n",
    "    * Memory: $\\{\\hat{Y}_k\\} \\in \\mathbb{R}^{K \\times M \\times N}$.\n",
    "* The filtered signal is then given by $\\hat{Y} = \\sum_k c_k \\hat{Y}_k$\n",
    "    * This operation requires $O(KMN)$ operations where filtering in the Fourier domain costs $O(MN)$. But there will be many of these operations during SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rescale_L(L):\n",
    "    \"\"\"Rescale the Laplacian eigenvalues in [-1,1].\"\"\"\n",
    "    M, M = L.shape\n",
    "    I = scipy.sparse.identity(M, format='csr')\n",
    "    if normalized_laplacian:\n",
    "        return L - I\n",
    "    else:\n",
    "        return L / lmax * 2 - I\n",
    "    \n",
    "def cheby_basis_eval(L, X, K):\n",
    "    \"\"\"Return T_k X where T_k are the Chebyshev polynomials of order up to K.\n",
    "    Complexity is O(KMN).\"\"\"\n",
    "    M, N = X.shape\n",
    "    L = rescale_L(L)\n",
    "    # Y = T @ X: MxM @ MxN.\n",
    "    Y = np.empty((K, M, N))\n",
    "    # Y_0 = T_0 X = I X = X.\n",
    "    Y[0,...] = X\n",
    "    # Y_1 = T_1 X = L X.\n",
    "    if K > 1:\n",
    "        Y[1,...] = L.dot(X)\n",
    "    # Y_k = 2 L Y_k-1 - Y_k-2.\n",
    "    for k in range(2, K):\n",
    "        Y[k,...] = 2 * L.dot(Y[k-1,...]) - Y[k-2,...]\n",
    "    return Y\n",
    "np.testing.assert_allclose(cheby_basis_eval(LL, X, 1)[0,...], X)\n",
    "\n",
    "def filter_recursive(Yhat, c):\n",
    "    K, M, N = Yhat.shape\n",
    "    Yhat.shape = (K, M*N)\n",
    "    Y = c @ Yhat\n",
    "    Y.shape = (M, N)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clenshaw's method\n",
    "\n",
    "* An algorithm which can evaluate any 3-relations recusive polynomial given the coefficients.\n",
    "* The complexity of filtering with Clenshaw is $O(KMN)$, exactly the same as the standard Chebyshev recursion.\n",
    "* The advantage of Chebyshev recursion is the possibility to store the $\\hat{Y}_k$, which diminishes the computation cost by two for successive evaluation with different coefficients.\n",
    "* Is the approximation error of this method smaller ? Otherwise I don't see why numerical packages like scipy uses it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_clenshaw(x, c):\n",
    "    K = len(c)\n",
    "    b2 = 0\n",
    "    b1 = c[K-1] * np.ones(x.shape) if K >= 2 else 0\n",
    "    for k in range(K-2, 0, -1):\n",
    "        b = c[k] + 2 * x * b1 - b2\n",
    "        b2, b1 = b1, b\n",
    "    return c[0] + x * b1 - b2\n",
    "\n",
    "def test_basis(K, N=100):\n",
    "    x = np.linspace(-1, 1, N)\n",
    "    T = np.empty((N, K))\n",
    "    for k in range(K):\n",
    "        c = np.zeros(k+1)\n",
    "        c[k] = 1\n",
    "        T[:,k] = eval_clenshaw(x, c)\n",
    "    np.testing.assert_allclose(T, cheby_basis(K, x))\n",
    "test_basis(50)\n",
    "\n",
    "def filter_clenshaw(L, X, c):\n",
    "    K = len(c)\n",
    "    L = rescale_L(L)\n",
    "    B2 = 0\n",
    "    B1 = c[K-1] * X if K >= 2 else np.zeros(X.shape)\n",
    "    for k in range(K-2, 0, -1):\n",
    "        B = c[k] * X + 2 * L.dot(B1) - B2\n",
    "        B2, B1 = B1, B\n",
    "    return c[0] * X + L.dot(B1) - B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing polynomials evaluation and filtering\n",
    "\n",
    "* The first method seems faster because the Laplacian is already diagonalized.\n",
    "* Clenshaw seems faster because it doesn't keep the intermediate result.\n",
    "* Execution times for M=1000, N=200, K=15: 1617ms 36ms 44ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_compare(c):\n",
    "    t_start = time.process_time()\n",
    "    T = cheby_basis(len(c), x)\n",
    "    Y1 = filter_full(X, T @ c)\n",
    "    t_full = (time.process_time() - t_start) * 1000\n",
    "    \n",
    "    t_start = time.process_time()\n",
    "    Yhat = cheby_basis_eval(LL, X, len(c))\n",
    "    Y2 = filter_recursive(Yhat, c)\n",
    "    t_cheby = (time.process_time() - t_start) * 1000\n",
    "    np.testing.assert_allclose(Y1, Y2, atol=tol)\n",
    "    \n",
    "    t_start = time.process_time()\n",
    "    Y2 = filter_clenshaw(LL, X, c)\n",
    "    t_clenshaw = (time.process_time() - t_start) * 1000\n",
    "    np.testing.assert_allclose(Y1, Y2, atol=tol)\n",
    "    \n",
    "    print('Execution times: {:1.0f}ms {:1.0f}ms {:1.0f}ms'.format(t_full, t_cheby, t_clenshaw))\n",
    "\n",
    "test_compare(np.array([1]))\n",
    "test_compare(np.array([1,0,0,0]))\n",
    "test_compare(np.array([0,1,0,0]))\n",
    "test_compare(np.array([0,0,1,0]))\n",
    "test_compare(np.array([0,0,0,1]))\n",
    "test_compare(np.random.uniform(0, 5, size=100))\n",
    "test_compare(c_cg)\n",
    "test_compare(c_co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimality condition\n",
    "\n",
    "Minimize $L = \\frac{1}{N} \\|\\hat{Y} - Y\\|_F^2$ where $\\hat{Y} = \\sum_k c_k \\hat{Y}_k$.\n",
    "\n",
    "* Rewrite as $L = \\frac{1}{N} \\|\\bar{Y} c - \\bar{y}\\|_F^2$ where $\\bar{y}$ is the vectorized matrix $Y$ and the $k^\\text{th}$ column of $\\bar{Y}$ is the vectorized matrix $\\hat{Y}_k$.\n",
    "* Gradient $\\nabla_c L = \\frac{2}{N} \\bar{Y}^T (\\bar{Y} c - \\bar{y})$\n",
    "* Optimality condition $\\bar{Y} c = \\bar{y}$\n",
    "    * Largely over-determined as $K << MN$; $\\bar{y} \\in \\mathbb{R}^{NM}$ and $c \\in \\mathbb{R}^K$.\n",
    "\n",
    "1. Compute these coefficients without the basis, thus without the eigenvalues.\n",
    "    * And we avoid the $O(M^3)$ EVD.\n",
    "    * Avoid the EVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize(Yhat, Y):\n",
    "    K, M, N = Yhat.shape\n",
    "    return Yhat.reshape((K, M*N)), Y.reshape((M*N))\n",
    "\n",
    "def Lcr(c):\n",
    "    Yhat = cheby_basis_eval(LL, X, len(c))\n",
    "    return np.linalg.norm(filter_recursive(Yhat, c) - Y, ord='fro')**2 / N\n",
    "\n",
    "def dLcr(X, Y, c):\n",
    "    Yhat = cheby_basis_eval(LL, X, len(c))\n",
    "    Ybar, ybar = vectorize(Yhat, Y)\n",
    "    return 2 / N * (c @ Ybar - ybar) @ Ybar.T\n",
    "\n",
    "def cheby_coeff_opt(X, Y, K):\n",
    "    Yhat = cheby_basis_eval(LL, X, len(c))\n",
    "    Ybar, ybar = vectorize(Yhat, Y)\n",
    "    return np.linalg.lstsq(Ybar.T, ybar)[0]\n",
    "\n",
    "t_start = time.process_time()\n",
    "c_cro = cheby_coeff_opt(X, Y, K)\n",
    "print('Execution time: {:1.0f}ms'.format((time.process_time() - t_start) * 1000))\n",
    "\n",
    "np.testing.assert_allclose(Lcr(c_cro), L(T @ c_cro), atol=tol)\n",
    "assert Lcr(c_cro) < Lcr(c_cg) + tol\n",
    "assert Lcr(c_cro) < Lcr(c_co) + tol\n",
    "if pol_order_is_sufficient:\n",
    "    np.testing.assert_allclose(Lcr(c_cro), M * eps**2, rtol=2e-2, atol=tol)\n",
    "if eps <= 0 and pol_order_is_sufficient:\n",
    "    np.testing.assert_allclose(Lcr(c_cro), Lcr(c_co), atol=tol)\n",
    "np.testing.assert_allclose(dLcr(X, Y, c_cro), 0, atol=tol)\n",
    "assert np.linalg.norm(dLcr(X, Y, c_cro)) < np.linalg.norm(dLcr(X, Y, c_cg)) + tol\n",
    "assert np.linalg.norm(dLcr(X, Y, c_cro)) < np.linalg.norm(dLcr(X, Y, c_co)) + tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c0 = np.random.uniform(0, 1, K)\n",
    "c_crs = sgd_plot_convergence(c0, Lcr, dLcr, [[.01, 100]], conv=1e-3, maxit=100)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: learned filters\n",
    "\n",
    "* Fast decay of the coefficients: good for approximation.\n",
    "* The *ground truth* and *optimal* coefficients are similar, given a sufficiently high order $K$. Otherwize the *optimal* are better.\n",
    "\n",
    "\n",
    "* The optimal solutions `c_co` and `c_cro` are close and have the smallest loss.\n",
    "* The SGD solutions `c_cs` and `c_crs` are close while a bit less accurate.\n",
    "    * Probably the convergence isn't that great.\n",
    "    * They approximate the hight frequencies the least accurately.\n",
    "* The ground truth solution lies in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_coefficients(['c_crs', 'c_cro', 'c_cs', 'c_co', 'c_cg'])\n",
    "plot_filters(['T @ c_crs', 'T @ c_cro', 'c_o'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
