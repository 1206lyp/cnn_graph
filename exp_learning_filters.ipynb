{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: learning graph filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random kNN graph: $W \\in \\mathbb{R}^{M \\times M} \\rightarrow L \\rightarrow U, \\Lambda$.\n",
    "2. Random graph signals: $X = \\{x_i\\}_{i=1}^N \\in \\mathbb{R}^{M \\times N}$.\n",
    "3. Linear mapping: $f(x_i, c) = U \\operatorname{diag}(c) U^T x_i$.\n",
    "4. Noisy target signals: $Y = \\{y_i\\}_{i=1}^N \\in \\mathbb{R}^{M \\times N}, y_i = f(x_i, c_{gt}) + \\mathcal{N}_M(0,\\epsilon)$.\n",
    "    1. With randomly generated coefficients $c_{gt} \\sim \\mathcal{N}_M(0,1)$.\n",
    "5. Convex and smooth loss function: $L = \\frac{1}{N} \\sum_{i=1}^N \\|f(x_i, c) - y_i\\|_2^2 = \\frac{1}{N} \\|U \\operatorname{diag}(c) U^TX - Y\\|_F^2$.\n",
    "    1. Gradient: $\\nabla_{c} L = \\frac{2}{N} \\left(U^T X \\circ ( c \\circ U^T X - U^T Y ) \\right) 1_N$.\n",
    "6. Optimization: $c^* = \\operatorname{arg min}_c L(c)$.\n",
    "7. Verification.\n",
    "    1. $c^*$ should converge to $c_{gt}$.\n",
    "    2. The loss $L(c^*)$ should converge to $L(c_{gt})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random, time\n",
    "import numpy as np\n",
    "import scipy.sparse, scipy.sparse.linalg, scipy.spatial.distance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "tol = 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Setting\n",
    "\n",
    "### Graph\n",
    "\n",
    "* A completely random graph is not smooth at all and will thus have a large spectral gap, i.e. $\\lambda_1 >> \\lambda_0$.\n",
    "* A grid, on the contrary, is very regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 100  # nodes\n",
    "k = 4  # edges per vertex\n",
    "\n",
    "def graph_random():\n",
    "    \"\"\"Random connections and weights.\"\"\"\n",
    "    I = np.arange(0, M).repeat(k)\n",
    "    J = np.random.randint(0, M, M*k)\n",
    "    V = np.random.uniform(0, 1, M*k)\n",
    "    return scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n",
    "\n",
    "def graph_grid():\n",
    "    \"\"\"Construct a kNN graph aranged on a 2D grid.\"\"\"\n",
    "    \n",
    "    # Construct a grid.\n",
    "    m = np.int(np.sqrt(M))\n",
    "    x = np.linspace(0,1,m)\n",
    "    y = np.linspace(0,1,m)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = np.empty((M,2))\n",
    "    z[:,0] = xx.reshape(M)\n",
    "    z[:,1] = yy.reshape(M)\n",
    "\n",
    "    # Compute pairwise distances.\n",
    "    d = scipy.spatial.distance.pdist(z, 'euclidean')\n",
    "    d = scipy.spatial.distance.squareform(d)\n",
    "\n",
    "    # k-NN graph.\n",
    "    idx = np.argsort(d)[:,1:k+1]\n",
    "    d.sort()\n",
    "    d = d[:,1:k+1]\n",
    "\n",
    "    # Weights.\n",
    "    sigma2 = np.mean(d[:,-1])**2\n",
    "    d = np.exp(- d**2 / sigma2)\n",
    "\n",
    "    # Weight matrix.\n",
    "    I = np.arange(0, M).repeat(k)\n",
    "    J = idx.reshape(M*k)\n",
    "    V = d.reshape(M*k)\n",
    "    return scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n",
    "\n",
    "W = graph_grid()\n",
    "\n",
    "# No self-connections.\n",
    "W.setdiag(0)\n",
    "\n",
    "# Non-directed graph.\n",
    "bigger = W.T > W\n",
    "W = W - W.multiply(bigger) + W.T.multiply(bigger)\n",
    "del bigger\n",
    "assert np.abs(W - W.T).mean() < tol\n",
    "\n",
    "# CSR sparse matrix format for efficient multiplications.\n",
    "W = W.tocsr()\n",
    "W.eliminate_zeros()\n",
    "\n",
    "print(\"{} > {} edges\".format(W.nnz, M*k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $L^\\text{unnormalized} = D - W$\n",
    "* $L^\\text{normalized} = I - D^{-1/2} W D^{-1/2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normalized_laplacian = True\n",
    "\n",
    "def laplacian(W, normalized=True):\n",
    "    \"\"\"Return the Laplacian of the weigth matrix.\"\"\"\n",
    "    \n",
    "    # Degree matrix.\n",
    "    d = W.sum(axis=0)\n",
    "\n",
    "    # Laplacian matrix.\n",
    "    if not normalized:\n",
    "        D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "        return D - W\n",
    "    else:\n",
    "        d = 1 / np.sqrt(d)\n",
    "        D = scipy.sparse.diags(d.A.squeeze(), 0)\n",
    "        I = scipy.sparse.identity(M, dtype=D.dtype)\n",
    "        return I - D * W * D\n",
    "\n",
    "LL = laplacian(W, normalized_laplacian)\n",
    "assert np.abs(LL - LL.T).mean() < tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L = U^T \\Lambda U$ where $\\Lambda$ is a diagonal matrix of eigenvalues.\n",
    "Compare the results of four algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sort(lamb, U):\n",
    "    idx = lamb.argsort()\n",
    "    return lamb[idx], U[:,idx]\n",
    "\n",
    "lamb, U = np.linalg.eig(LL.toarray())\n",
    "lamb, U = sort(lamb, U)\n",
    "\n",
    "lamb_, U_ = np.linalg.eigh(LL.toarray())\n",
    "np.testing.assert_allclose(lamb_, lamb, atol=tol)\n",
    "np.testing.assert_allclose(np.abs(U_), np.abs(U), atol=tol)\n",
    "\n",
    "lamb_, U_ = scipy.sparse.linalg.eigs(LL, k=M-2, which='SM')\n",
    "lamb_, U_ = sort(lamb_, U_)\n",
    "np.testing.assert_allclose(lamb[:-2], lamb_, atol=tol)\n",
    "np.testing.assert_allclose(np.abs(U[:,:-2]), np.abs(U_), atol=tol)\n",
    "\n",
    "lamb_, U_ = scipy.sparse.linalg.eigsh(LL, k=M-1, which='SM')\n",
    "np.testing.assert_allclose(lamb[:-1], lamb_, atol=tol)\n",
    "np.testing.assert_allclose(np.abs(U[:,:-1]), np.abs(U_), atol=tol)\n",
    "\n",
    "del lamb_, U_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper-bound approximation of the spectrum.\n",
    "\n",
    "* Computed by the Implicitly Restarted Lanczos Method (IRLM), which is a reduction of a variant of the Arnoldi iteration. It is faster than the Power method.\n",
    "* Normalized graph Laplacian has a bounded spectrum $0 \\leq \\lambda \\leq 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lmax = scipy.sparse.linalg.eigsh(LL, k=1, which='LM', return_eigenvectors=False)[0]\n",
    "if normalized_laplacian:\n",
    "    assert lmax <= 2\n",
    "print('Spectrum: [{:1.2e}, {:1.2e}]'.format(lamb[0], lmax))\n",
    "np.testing.assert_allclose(lamb[0], 0, atol=tol)\n",
    "np.testing.assert_allclose(lamb[-1], lmax, atol=tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth graph filter\n",
    "\n",
    "Linear mapping $f(x_i, c) = U C U^T x_i$, $C$ is the diagonal matrix $C = \\operatorname{diag}(c)$, i.e. $c = C 1_M$.\n",
    "\n",
    "* Parametrized low-pass filter coefficients $(c_{gt})_i = \\operatorname{e}^{-t \\lambda_i}$\n",
    "* Random filter coefficients $c_{gt} \\sim \\mathcal{N}_M(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parametrized = True\n",
    "\n",
    "if parametrized:\n",
    "    def g(x, t=.5):\n",
    "        return np.sin(2 * (x-2)**2)\n",
    "        return np.exp(-t * x)\n",
    "    c_gt = g(lamb)\n",
    "    #assert np.all(c_gt[:-1] - c_gt[1:] > 0)\n",
    "else:\n",
    "    c_gt = np.random.normal(0, 1, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signals\n",
    "\n",
    "* Random input signals $X \\sim \\mathcal{N}_{M \\times N}(0,1)$\n",
    "  * Low-pass signals ?\n",
    "* Noisy target signals $y_i = f(x_i, c_{gt}) + \\mathcal{N}_M(0,\\epsilon)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 200  # signals\n",
    "eps = 0.1  # noise\n",
    "\n",
    "X = np.random.normal(0, 1, (M,N))\n",
    "Y = U @ np.diag(c_gt) @ U.T @ X + (np.random.normal(0, eps, (M,N)) if eps > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametrized filter learning\n",
    "\n",
    "### Loss function\n",
    "\n",
    "* Loss function $L = \\frac{1}{N} \\sum_{i=1}^N \\|f(x_i, c) - y_i\\|_2^2 = \\frac{1}{N} \\|UCU^TX - Y\\|_F^2$.\n",
    "    * Spectral domain: $L = \\frac{1}{N} \\| C U^T X - U^T Y \\|_F^2$.\n",
    "    * Independant coefficients: $L = \\frac{1}{N} \\sum_{i=1}^M \\| c_i (U^T X)_{i,\\cdot} - (U^T Y)_{i,\\cdot} \\|_2^2$.\n",
    "    * Convex and smooth w.r.t. $c$.\n",
    "* Gradient:\n",
    "    * Independant coefficients: $\\nabla_{c_i} L = \\frac{2}{N} ( c_i (U^T X)_{i,\\cdot} - (U^T Y)_{i,\\cdot} ) (X^T U)_{\\cdot,i}$.\n",
    "    * $\\nabla_{c} L = \\frac{2}{N} \\left(U^T X \\circ ( c \\circ U^T X - U^T Y ) \\right) 1_N$.\n",
    "* Optimization $c^* = \\operatorname{arg min}_{c} L(c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def L(c):\n",
    "    M, N = X.shape\n",
    "    return np.linalg.norm(U @ np.diag(c) @ U.T @ X - Y, 'fro')**2 / N\n",
    "\n",
    "print(\"L(c_gt) = {}\".format(L(c_gt)))\n",
    "np.testing.assert_allclose(L(c_gt), M * eps**2, 2e-2)\n",
    "\n",
    "def dL(X, Y, c, variant=None):\n",
    "    M, N = X.shape\n",
    "    A = U.T @ X\n",
    "    B = U.T @ Y\n",
    "    # Speed: v3 >> v1 > v2.\n",
    "    if variant is 1:\n",
    "        return 2 / N * np.diag(A @ (A.T @ np.diag(c) - B.T))\n",
    "    elif variant is 2:\n",
    "        dc = np.empty(M)\n",
    "        for i in range(M):\n",
    "            dc[i] = 2 / N * (c[i] * A[i,:] - B[i,:]) @ A.T[:,i]\n",
    "        return dc\n",
    "    else:\n",
    "        # Speed: .sum(axis=1) is faster than *np.ones(N).y\n",
    "        return 2 / N * (A * (c[:,np.newaxis] * A - B)).sum(axis=1)\n",
    "\n",
    "print(\"|dL(c_gt)| = {}\".format(np.linalg.norm(dL(X, Y, c_gt))))\n",
    "# Gradient should be null at the global minimum. With noise, c_gt is not necessary the optimum.\n",
    "if eps <= 0:\n",
    "    np.testing.assert_allclose(dL(X, Y, c_gt), 0, atol=tol)\n",
    "np.testing.assert_allclose(dL(X, Y, c_gt), dL(X, Y, c_gt, 1))\n",
    "np.testing.assert_allclose(dL(X, Y, c_gt), dL(X, Y, c_gt, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: optimality condition\n",
    "\n",
    "* Only possible because $L$ is convex and smooth.\n",
    "* Optimality condition $\\nabla_c L = 0$ gives $(U^T X \\circ U^T X) 1_N \\circ c = (U^T X \\circ U^T Y) 1_N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tstart = time.process_time()\n",
    "A = U.T @ X\n",
    "B = U.T @ Y\n",
    "c_opt = (A * B).sum(axis=1) / (A * A).sum(axis=1)\n",
    "print('Execution time: {:1.0f}ms'.format((time.process_time() - tstart) * 1000))\n",
    "\n",
    "print(\"L(c_opt) = {}\".format(L(c_opt)))\n",
    "assert L(c_opt) < L(c_gt) + tol\n",
    "print(\"|dL(c_opt)| = {}\".format(np.linalg.norm(dL(X, Y, c_opt))))\n",
    "assert np.linalg.norm(dL(X, Y, c_opt)) < np.linalg.norm(dL(X, Y, c_gt))\n",
    "np.testing.assert_allclose(dL(X, Y, c_opt), 0, atol=tol)\n",
    "if eps <= 0:\n",
    "    np.testing.assert_allclose(c_opt, c_gt, atol=tol)\n",
    "    np.testing.assert_allclose(L(c_opt), L(c_gt), atol=tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: stochastic (mini-batch) gradient descent\n",
    "\n",
    "* Works also for $L$ which are non-smooth (with sub-gradient) or non-convex.\n",
    "* Idea: descend the gradient of the loss function.\n",
    "* Efficiency: compute the gradient $\\nabla_c L$ with a sub-set (mini-batch) of the training data.\n",
    "    * Extreme case: one sample at a time. Very inefficient.\n",
    "* Update rule (gradient descent) $c^{n+1} = c^n - \\lambda_n \\nabla_c L$.\n",
    "* Note: objective (loss on training set) and error (on validation set) are usually evaluated after each epoch. The algorithm is thus stopped after a maximum number of epochs rather than iterations.\n",
    "* Hyper-parameters.\n",
    "    * Learning rate (step size) $\\lambda_n$. Bigger the batch size, smaller the learning rate.\n",
    "        * Tradeoff.\n",
    "            * Small: progress is steady but slow.\n",
    "            * Big: risks of oscillations or divergence.\n",
    "        * There are tricks, e.g. vanishing step (like simulated annealing).\n",
    "    * Size of the mini-batch.\n",
    "        * We want the one who minimizes the *training time*.\n",
    "        * Trade-off: should be limited by the available memory, somewhere around 100.\n",
    "            * Larger is more stable, but computationnaly more expensive.\n",
    "            * Smaller demands more accesses to memory, which is slow.\n",
    "            * Larger exploits the parallelism of modern hardware architectures (SIMD on CPU, GPU).\n",
    "        * Extreme cases:\n",
    "            * $1$: stochastic gradient descent.\n",
    "            * $N$: gradient descent.\n",
    "    * Stopping criterion.\n",
    "        * Convergence of the loss function $L$.\n",
    "        * Convergence of the parameters $c$.\n",
    "        * Maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(c0, L, dL, learning_rate=.1, batch_size=100, conv=1e-3, maxit=100, window=10):\n",
    "    \"\"\"Stochastic (mini-batch) gradient descent.\"\"\"\n",
    "    indices = []\n",
    "    c = c0\n",
    "    loss = [L(c)]\n",
    "    \n",
    "    def stop(loss):\n",
    "        \"\"\"Stop after convergence of the loss.\"\"\"\n",
    "        if len(loss) > maxit:\n",
    "            return True\n",
    "        #elif np.linalg.norm(dL(X, Y, c)) < conv:\n",
    "            #return True\n",
    "        elif len(loss) >= 2 * window:\n",
    "            avg1 = np.mean(loss[-window:])\n",
    "            avg2 = np.mean(loss[-2*window:-window])\n",
    "            return True if avg2 - avg1 < conv else False\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    while not stop(loss):\n",
    "        \n",
    "        # Be sure to have used all the samples before using one a second time.\n",
    "        if len(indices) < batch_size:\n",
    "            new_indices = np.arange(N)\n",
    "            np.random.shuffle(new_indices)\n",
    "            indices.extend(new_indices)\n",
    "        idx = indices[:batch_size]\n",
    "        del indices[:batch_size]\n",
    "        \n",
    "        c -= learning_rate * dL(X[:,idx], Y[:,idx], c)\n",
    "        loss.append(L(c))\n",
    "        \n",
    "    return c, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Observations:\n",
    "* Noise: why don't we find the same loss as the ground truth, but the same as linear programming ?\n",
    "    * The gradient was incorrectly set to $\\nabla_c L = \\frac{2}{N} U^T X (X^T U c - Y^T U 1_M)$.\n",
    "* More samples, e.g. $N=2000$: why don't we find the same loss as the linear programm ?\n",
    "    * Learning rate too high.\n",
    "* The spectral gap $\\lambda_1$ is large for a random graph.\n",
    "* Without noise, the recovered filter is exact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plots(c0, L_, dL_, params, conv, maxit, cs, T=None):\n",
    "    \n",
    "    fig_conv = plt.figure(figsize=(15,5))\n",
    "    fig_filt = plt.figure(figsize=(15,5))\n",
    "    ax_conv = fig_conv.add_subplot(1,1,1)\n",
    "    ax_filt = fig_filt.add_subplot(1,1,1)\n",
    "\n",
    "    def plot(learning_rate, batch_size):\n",
    "        tstart = time.process_time()\n",
    "        (c, loss) = sgd(np.array(c0), L_, dL_, learning_rate, batch_size, conv, maxit)\n",
    "        t = (time.process_time() - tstart) * 1000\n",
    "        label = 'rate={}, size={}, L(c)={:1.2e}, |dL(c)|={:1.2e}, time={:1.0f}ms'.format(\n",
    "            learning_rate, batch_size, L_(c), np.linalg.norm(dL_(X, Y, c)), t)\n",
    "        ax_conv.plot(loss, label=label)\n",
    "        ax_filt.plot(lamb, c if T is None else T @ c, '.-', label=label)\n",
    "\n",
    "    for param in params:\n",
    "        plot(param[0], param[1])\n",
    "\n",
    "    for c in cs:\n",
    "        label = '{0}, L({0})={1:1.2e}, |dL({0})|={2:1.2e})'.format(c, L(eval(c)), np.linalg.norm(dL(X,Y,eval(c))))\n",
    "        ax_conv.plot(L(eval(c)) * np.ones(maxit+1), label=label)\n",
    "        ax_filt.plot(lamb, eval(c), '.-', label=label)\n",
    "\n",
    "    ax_conv.set_title('Convergence, M={}, N={}, eps={}'.format(M, N, eps))\n",
    "    ax_conv.set_xlabel('iteration n')\n",
    "    ax_conv.set_ylabel('loss L(c^n)')\n",
    "    ax_conv.legend(loc='best')\n",
    "#    fig_conv.show()\n",
    "    \n",
    "    ax_filt.set_xlim(lamb[0], lamb[-1])\n",
    "    ax_filt.set_title('Filters, M={}, N={}, eps={}'.format(M, N, eps))\n",
    "    ax_filt.set_xlabel('frequency lamb')\n",
    "    ax_filt.set_ylabel('filter coefficients c')\n",
    "    ax_filt.legend(loc='best')\n",
    "#    fig_filt.show()\n",
    "    \n",
    "params = []\n",
    "params.append([0.2, 1])\n",
    "params.append([0.2, 5])\n",
    "params.append([0.2, 50])\n",
    "params.append([0.2, 100])\n",
    "params.append([0.6, 100])\n",
    "c0 = np.random.uniform(0, 1, M)\n",
    "plots(c0, L, dL, params, 1e-3, 40, ['c_opt', 'c_gt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter learning: truncated Chebyshev expansion\n",
    "\n",
    "* Use a $K$th order polynomial approximation of the filter.\n",
    "* Less free parameters: $K << M$.\n",
    "* Good approximation for smooth, i.e. localized, filters.\n",
    "\n",
    "### Basis of Chebyshev polynomials\n",
    "\n",
    "* Compute the Chebyshev basis $T$ of order $K$.\n",
    "* This basis will allow us to construct and observe the filter from the inferred polynomial coefficients.\n",
    "* The figure shows that we indeed generate the Chebyshev polynomials of the first kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 5\n",
    "\n",
    "def cheby_pol(K, x):\n",
    "    \"\"\"Return the Chebyshev basis of order K (composed of the\n",
    "    first K polynomials) evaluated at x. Polynomials are generated\n",
    "    by their recursive formulation.\"\"\"\n",
    "    T = np.empty((x.size, K))\n",
    "    T[:,0] = np.ones(x.size)\n",
    "    if K >= 2:\n",
    "        T[:,1] = x\n",
    "    for k in range(2, K):\n",
    "        T[:,k] = 2 * x * T[:,k-1] - T[:,k-2]\n",
    "    return T\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "x = np.linspace(-1,1,100)\n",
    "T = cheby_pol(K, x)\n",
    "for k in range(K):\n",
    "    ax.plot(x, T[:,k], label='T_{}'.format(k))\n",
    "ax.set_title('Chebyshev polynomials of the first kind')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('T_n(x)')\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1.1)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground-truth Chebyshev coefficients\n",
    "\n",
    "Given the filter $g$ with a vector $c_{gt} \\in \\mathbb{R}^M$ of evaluations, find the Chebyshev coefficients $c_{cgt} \\in \\mathbb{R}^K$. Truncated Chebyshev series closely approximate the minimax polynomial, i.e. $c_{cgt} \\approx \\operatorname{arg min}_c \\| c_{gt} - \\sum_k c_k T_k \\|_\\infty$ where $T_k$ is the Chebyshev polynomial of order $k$. Given that the polynomials form an orthogonal basis for $L^2([-1,1],\\frac{dy}{\\sqrt{1-y^r}})$, the coefficients can be retrieved by two methods.\n",
    "\n",
    "1. Analytical projection.\n",
    "    * $c_k = \\frac{2}{\\pi} \\int_0^\\pi \\cos(k\\theta) g( \\frac{\\lambda_{max}}{2} (\\cos(\\theta) + 1)) d\\theta$\n",
    "    * Need the analytic function.\n",
    "2. Numerical projection (discrete orthogonality condition).\n",
    "    * $c_k = \\frac{2}{K} \\sum_j g(x_j) T_k(x_j)$ where the $x_j$ are the $K$ Chebyshev nodes, because the approximation error is null only at these points.\n",
    "    * Need function evaluations at the Chebyshev nodes, but those only. Much less points than least mean square.\n",
    "\n",
    "In our setting, the generative filter is the function to learn. We have however access to some evaluations of the filter (at the eigenvalues of the Laplacian) via convex optimization of the loss function $L$ (described above). From those, given the Chebyshev basis, we can retrieve the coefficients that minimize the reconstruction error of this filter.\n",
    "\n",
    "Results:\n",
    "\n",
    "* Playing with the order $K$ shows that the approximation converges to the filter $g$.\n",
    "* The approximation constructed by minimizing the filter l2 reconstruction error is now longer a Chebyshev polynomial (there are error on the Chebyshev nodes) but it provides a smaller loss $L$ (our final measure of quality). It however requires the full Chebyshev basis, which requires the eigenvalues of the Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "\n",
    "def rescale(x, reverse=False):\n",
    "    \"\"\"Rescale the spectral domain to [-1,1].\"\"\"\n",
    "    old_min = -1\n",
    "    old_max = 1\n",
    "    new_min = lamb[0]\n",
    "    new_max = lamb[-1]\n",
    "    if reverse:\n",
    "        tmp_min, tmp_max = old_min, old_max\n",
    "        old_min, old_max = new_min, new_max\n",
    "        new_min, new_max = tmp_min, tmp_max\n",
    "    x = (x - old_min) / (old_max - old_min)\n",
    "    return x * (new_max - new_min) + new_min\n",
    "np.testing.assert_allclose(lamb, rescale(rescale(lamb, True)))\n",
    "\n",
    "def cheby_nodes(K):\n",
    "    \"\"\"Return the K Chebyshev nodes in [-1,1].\"\"\"\n",
    "    return np.cos(np.pi * (np.arange(K) + 1/2) / K)\n",
    "    \n",
    "def cheby_coeff(K, f):\n",
    "    \"\"\"Compute the coefficients of the Chebyshev polynomial approximation.\"\"\"\n",
    "    # Coefficients from discrete orthogonality condition.\n",
    "    # It can be done faster via the discrete cosine transform.\n",
    "    c = np.empty(K)\n",
    "    x = cheby_nodes(K)\n",
    "    T = cheby_pol(K, x)\n",
    "    for k in range(K):\n",
    "        c[k] = 2 / K * np.sum(f(x) * T[:,k])\n",
    "    c[0] /= 2\n",
    "    return c\n",
    "\n",
    "# Domain is [-1, 1].\n",
    "x = np.linspace(-1,1,100)\n",
    "x = rescale(lamb, True)\n",
    "f = lambda x: g(rescale(x))\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "np.testing.assert_allclose(f(x), c_gt)\n",
    "ax.plot(rescale(x), c_gt, '.-', label='target function, L={:1.2e}'.format(L(c_gt)))\n",
    "ax.plot(rescale(cheby_nodes(K)), f(cheby_nodes(K)), '.', markersize=15, label='Chebyshev nodes')\n",
    "\n",
    "c = cheby_coeff(K, f)\n",
    "T = cheby_pol(K, x)\n",
    "ax.plot(rescale(x), T @ c, '.-', label='discrete orthogonality condition, L={:1.2e}'.format(L(T @ c)))\n",
    "\n",
    "# The error should be zero at the Chebyshev nodes.\n",
    "np.testing.assert_allclose(f(cheby_nodes(K)), cheby_pol(K, cheby_nodes(K)) @ c)\n",
    "\n",
    "c = np.linalg.lstsq(T, c_gt)[0]\n",
    "ax.plot(rescale(x), T @ c, '.-', label='least mean square, L={:1.2e}'.format(L(T @ c)))\n",
    "\n",
    "ax.set_title('Chebyshev approximation of order {}'.format(K))\n",
    "ax.set_xlabel('Frequency')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.legend(loc='best')\n",
    "ax.set_xlim(lamb[0], lamb[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the polynomial order by filtering the data with Chebyshev approximations of order $1 \\leq k \\leq K$ and monitoring the reconstruction loss $L$.\n",
    "\n",
    "* The result shows that the approximation does indeed converge.\n",
    "* The approximation loss arrives at a plateau (the round-off error ?) given a high enough order.\n",
    "* As anticipated on the figure above, the coefficients provided by least square reconstruction have smaller loss than the *correct* ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 60\n",
    "loss_gt = np.empty((K))\n",
    "loss_opt = np.empty((K))\n",
    "for k in range(1, K+1):\n",
    "    T = cheby_pol(k, x)\n",
    "    c = cheby_coeff(k, f)\n",
    "    loss_gt[k-1] = L(T @ c)\n",
    "    c = np.linalg.lstsq(T, f(x))[0]\n",
    "    loss_opt[k-1] = L(T @ c)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.semilogy(range(1,K+1), loss_gt, label='L(T @ c_cgt)')\n",
    "ax.semilogy(range(1,K+1), loss_opt, label='L(T @ c_copt)')\n",
    "ax.semilogy(L(c_gt) * np.ones(K+1), label='L(c_gt)')\n",
    "ax.semilogy(L(c_opt) * np.ones(K+1), label='L(c_opt)')\n",
    "ax.set_title('Loss due to Chebyshev approximation')\n",
    "ax.set_xlabel('Polynomial order')\n",
    "ax.set_ylabel('Loss L')\n",
    "ax.set_xlim(1, K)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the polynomial order $K$ and compute the basis $T$ with their associate coefficients $c_{cgt}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 15\n",
    "c_cgt = cheby_coeff(K, f)\n",
    "T = cheby_pol(K, x)\n",
    "\n",
    "# If the order is sufficient for a perfect (as good as c_gt) reconstruction (test only).\n",
    "pol_order_is_sufficient = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "* Independant coefficients: $L = \\frac{1}{N} \\sum_{i=1}^M \\| (Tc)_i (U^T X)_{i,\\cdot} - (U^T Y)_{i,\\cdot} \\|_2^2$.\n",
    "* $L = \\frac{1}{N} \\| Tc \\circ U^T X - U^T Y \\|_2^2$.\n",
    "* $\\nabla_{c} L = \\frac{2}{N} \\left(T^T \\left( U^T X \\circ ( Tc \\circ U^T X - U^T Y ) \\right) \\right) 1_N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Lc(c):\n",
    "    M, N = X.shape\n",
    "    return np.linalg.norm(U @ np.diag(T @ c) @ U.T @ X - Y, 'fro')**2 / N\n",
    "\n",
    "print(\"L(c_cgt) = {}\".format(Lc(c_cgt)))\n",
    "np.testing.assert_allclose(Lc(c_cgt), L(T @ c_cgt), atol=tol)\n",
    "if pol_order_is_sufficient:\n",
    "    np.testing.assert_allclose(Lc(c_cgt), M * eps**2, 2e-2)\n",
    "    np.testing.assert_allclose(Lc(c_cgt), L(c_gt), atol=tol)\n",
    "\n",
    "def dLc(X, Y, c):\n",
    "    M, N = X.shape\n",
    "    A = U.T @ X\n",
    "    B = U.T @ Y\n",
    "    return 2 / N * T.T @ (A * ((T @ c)[:,np.newaxis] * A - B)).sum(axis=1)\n",
    "\n",
    "print('|dL(c_cgt)| = {}'.format(np.linalg.norm(dLc(X, Y, c_cgt))))\n",
    "# Gradient should be null at the global minimum. With noise, c_gt is not necessary the optimum.\n",
    "if eps <= 0 and pol_order_is_sufficient:\n",
    "    np.testing.assert_allclose(dLc(X, Y, c_cgt), 0, atol=tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: optimality condition\n",
    "\n",
    "* Given the signals $X$, $Y$ and the Chebyshev basis $T$, find the Chebyshev coefficients $c_{copt}$.\n",
    "* Optimality condition $\\nabla_c L = 0$ gives $(U^T X \\circ U^T X) 1_N \\circ Tc = (U^T X \\circ U^T Y) 1_N$.\n",
    "* Why do we not always reach the minimum, i.e. $\\nabla_c L = 0$ ? E.g. for small polynomial orders, when we are not able to sufficiently approximate the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_copt = np.linalg.lstsq(T, c_opt)[0]\n",
    "print(\"L(c_copt) = {}\".format(Lc(c_copt)))\n",
    "assert Lc(c_copt) < Lc(c_cgt) + tol\n",
    "print('|dL(c_copt)| = {}'.format(np.linalg.norm(dLc(X, Y, c_copt))))\n",
    "assert np.linalg.norm(dLc(X, Y, c_copt)) < np.linalg.norm(dLc(X, Y, c_cgt))\n",
    "#np.testing.assert_allclose(dLc(X, Y, c_copt), 0, atol=tol)\n",
    "if eps <= 0 and pol_order_is_sufficient:\n",
    "    np.testing.assert_allclose(c_copt, c_cgt, atol=tol)\n",
    "    np.testing.assert_allclose(Lc(c_copt), Lc(c_cgt), atol=tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fast decay of the coefficients: good for approximation.\n",
    "* The *ground truth* and *optimal* coefficients are similar, given a sufficiently high order $K$. Otherwize the *optimal* are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.semilogy(abs(c_cgt), '.', label='c_cgt')\n",
    "ax.semilogy(abs(c_copt), 'x', label='c_copt')\n",
    "ax.set_title('Chebyshev coefficients')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "* Why |dL(c)| does not converge to the null vector ? There should be no gradient at the optimum.\n",
    "* Convergence seems harder than before.\n",
    "* The coefficients c_copt always provide a smallest loss than c_cgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [[0.005, 100]]\n",
    "c0 = np.random.uniform(0, 1, K)\n",
    "plots(c0, Lc, dLc, params, 1e-3, 80, ['T @ c_cgt', 'T @ c_copt', 'c_opt'], T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
